{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df1d7ebd-b7b7-4ada-b8bc-99ca9e8e7e3a",
   "metadata": {},
   "source": [
    "# Ensemble models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3925fb81-981b-4cf2-83e4-f49cb1ced6ee",
   "metadata": {},
   "source": [
    "## Hippocampus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8067b1cd-efdb-4a71-81d7-78a53f7056a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Methods:\n",
      "Simple Average Ensemble - MSE: 0.0707, Correlation: 0.1623\n",
      "Weighted Average Ensemble - MSE: 0.1992, Correlation: 0.0777\n",
      "Non-linear Weighted Average - MSE: 0.1021, Correlation: 0.1704\n",
      "Tuned Random Forest Meta-learner - MSE: 0.0844, Correlation: -0.0082\n",
      "Stacking Ensemble - MSE: 0.0816, Correlation: 0.0310\n",
      "\n",
      "Original Models:\n",
      "DenseNet - MSE: 0.0753, Correlation: 0.1425\n",
      "PointNet - MSE: 0.0779, Correlation: 0.0036\n",
      "Random Forest - MSE: 0.0700, Correlation: 0.1790\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr, randint\n",
    "from scipy import linalg\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess_data():\n",
    "    densenet_df = pd.read_csv('densenet_hippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "    pointnet_df = pd.read_csv('pointnet_hippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "    random_forest_df = pd.read_csv('random_forest_pyradiomics_hippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "    X = np.hstack((\n",
    "        densenet_df.iloc[:, 2:17].values,\n",
    "        pointnet_df.iloc[:, 2:17].values,\n",
    "        random_forest_df.iloc[:, 2:17].values\n",
    "    ))\n",
    "    y = densenet_df['Actual'].values\n",
    "\n",
    "    return X, y, densenet_df, pointnet_df, random_forest_df\n",
    "\n",
    "# Ensemble methods\n",
    "def simple_average_ensemble(X):\n",
    "    return np.mean(X, axis=1)\n",
    "\n",
    "def weighted_average_ensemble(X_train, y_train, X_test):\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    return lr_model.predict(X_test)\n",
    "\n",
    "def non_linear_weighted_average(X_train, y_train, X_test):\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    return mlp.predict(X_test)\n",
    "\n",
    "def tune_rf_meta_learner(X_train, y_train):\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(5, 50),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 10)\n",
    "    }\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n",
    "                                   n_iter=100, cv=5, random_state=42, n_jobs=-1)\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    return rf_random.best_estimator_\n",
    "\n",
    "def stacking_ensemble(X_train, y_train, X_test):\n",
    "    models = [\n",
    "        RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        SVR(kernel='rbf')\n",
    "    ]\n",
    "    \n",
    "    second_level_train = np.column_stack([\n",
    "        cross_val_predict(model, X_train, y_train, cv=5) \n",
    "        for model in models\n",
    "    ])\n",
    "    second_level_test = np.column_stack([\n",
    "        model.fit(X_train, y_train).predict(X_test) \n",
    "        for model in models\n",
    "    ])\n",
    "    \n",
    "    # Use Ridge regression as the meta-learner\n",
    "    alpha = 1.0\n",
    "    n_samples, n_features = second_level_train.shape\n",
    "    \n",
    "    # Add a small value to the diagonal for numerical stability\n",
    "    A = np.dot(second_level_train.T, second_level_train)\n",
    "    A.flat[::n_features + 1] += alpha + 1e-10\n",
    "    \n",
    "    b = np.dot(second_level_train.T, y_train)\n",
    "    \n",
    "    # Solve the linear system\n",
    "    try:\n",
    "        coef = linalg.solve(A, b, assume_a='pos')\n",
    "    except TypeError:\n",
    "        # For older versions of SciPy\n",
    "        coef = linalg.solve(A, b, sym_pos=True)\n",
    "    \n",
    "    # Make predictions\n",
    "    return np.dot(second_level_test, coef)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    correlation, _ = pearsonr(y_true, y_pred)\n",
    "    return mse, correlation\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    X, y, densenet_df, pointnet_df, random_forest_df = load_and_preprocess_data()\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    simple_avg_scores = []\n",
    "    weighted_avg_scores = []\n",
    "    non_linear_weighted_scores = []\n",
    "    rf_meta_scores = []\n",
    "    stacking_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Simple average\n",
    "        simple_avg_pred = simple_average_ensemble(X_test)\n",
    "        simple_avg_scores.append(evaluate_model(y_test, simple_avg_pred))\n",
    "        \n",
    "        # Weighted average\n",
    "        weighted_avg_pred = weighted_average_ensemble(X_train, y_train, X_test)\n",
    "        weighted_avg_scores.append(evaluate_model(y_test, weighted_avg_pred))\n",
    "        \n",
    "        # Non-linear weighted average\n",
    "        non_linear_weighted_pred = non_linear_weighted_average(X_train, y_train, X_test)\n",
    "        non_linear_weighted_scores.append(evaluate_model(y_test, non_linear_weighted_pred))\n",
    "        \n",
    "        # Random Forest meta-learner (with hyperparameter tuning)\n",
    "        rf_model = tune_rf_meta_learner(X_train, y_train)\n",
    "        rf_pred = rf_model.predict(X_test)\n",
    "        rf_meta_scores.append(evaluate_model(y_test, rf_pred))\n",
    "        \n",
    "        # Stacking ensemble\n",
    "        stacking_pred = stacking_ensemble(X_train, y_train, X_test)\n",
    "        stacking_scores.append(evaluate_model(y_test, stacking_pred))\n",
    "\n",
    "    # Calculate and print average scores\n",
    "    print(\"Ensemble Methods:\")\n",
    "    print(f\"Simple Average Ensemble - MSE: {np.mean([s[0] for s in simple_avg_scores]):.4f}, Correlation: {np.mean([s[1] for s in simple_avg_scores]):.4f}\")\n",
    "    print(f\"Weighted Average Ensemble - MSE: {np.mean([s[0] for s in weighted_avg_scores]):.4f}, Correlation: {np.mean([s[1] for s in weighted_avg_scores]):.4f}\")\n",
    "    print(f\"Non-linear Weighted Average - MSE: {np.mean([s[0] for s in non_linear_weighted_scores]):.4f}, Correlation: {np.mean([s[1] for s in non_linear_weighted_scores]):.4f}\")\n",
    "    print(f\"Tuned Random Forest Meta-learner - MSE: {np.mean([s[0] for s in rf_meta_scores]):.4f}, Correlation: {np.mean([s[1] for s in rf_meta_scores]):.4f}\")\n",
    "    print(f\"Stacking Ensemble - MSE: {np.mean([s[0] for s in stacking_scores]):.4f}, Correlation: {np.mean([s[1] for s in stacking_scores]):.4f}\")\n",
    "\n",
    "    # Evaluate original models\n",
    "    print(\"\\nOriginal Models:\")\n",
    "    for model_name, df in zip(['DenseNet', 'PointNet', 'Random Forest'], \n",
    "                              [densenet_df, pointnet_df, random_forest_df]):\n",
    "        avg_pred = df['Average_Prediction'].values\n",
    "        mse, correlation = evaluate_model(df['Actual'], avg_pred)\n",
    "        print(f\"{model_name} - MSE: {mse:.4f}, Correlation: {correlation:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0434b9-34c2-4af5-8d62-91d4991cf533",
   "metadata": {},
   "source": [
    "### MLP with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb24069b-7c28-42d2-a70b-5319cc095a86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 configurations by MSE:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0929\n",
      "   Correlation: 0.1766\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0958\n",
      "   Correlation: 0.2266\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1027\n",
      "   Correlation: 0.1901\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [256]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1071\n",
      "   Correlation: 0.2641\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1079\n",
      "   Correlation: 0.1123\n",
      "\n",
      "Top 5 configurations by Correlation:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1754\n",
      "   Correlation: 0.2890\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [256]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1071\n",
      "   Correlation: 0.2641\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1121\n",
      "   Correlation: 0.2472\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1095\n",
      "   Correlation: 0.2426\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0958\n",
      "   Correlation: 0.2266\n",
      "\n",
      "Best configuration by MSE:\n",
      "hidden_sizes: [128]\n",
      "dropout_rate: 0.5\n",
      "lr: 0.001\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "MSE: 0.0929\n",
      "\n",
      "Best configuration by Correlation:\n",
      "hidden_sizes: [128, 64, 32]\n",
      "dropout_rate: 0.5\n",
      "lr: 0.0005\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "Correlation: 0.2890\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a global seed\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "class MLPEnsemble(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(MLPEnsemble, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, patience, lr, weight_decay, seed):\n",
    "    set_seed(seed)  # Set seed for this training run\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).squeeze()\n",
    "    \n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "    predictions_np = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    \n",
    "    mse = mean_squared_error(y_np, predictions_np)\n",
    "    correlation, _ = pearsonr(y_np, predictions_np)\n",
    "    return mse, correlation\n",
    "\n",
    "def cross_validate_mlp(X, y, n_splits=5, **params):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    mse_scores = []\n",
    "    corr_scores = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    model_params = {\n",
    "        'input_size': X.shape[1],\n",
    "        'hidden_sizes': params['hidden_sizes'],\n",
    "        'output_size': params['output_size'],\n",
    "        'dropout_rate': params['dropout_rate']\n",
    "    }\n",
    "    train_params = {\n",
    "        'epochs': params['epochs'],\n",
    "        'patience': params['patience'],\n",
    "        'lr': params['lr'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'seed': GLOBAL_SEED\n",
    "    }\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        \n",
    "        model = MLPEnsemble(**model_params)\n",
    "        model = train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, **train_params)\n",
    "        \n",
    "        mse, corr = evaluate_model(model, X_val_tensor, y_val_tensor)\n",
    "        mse_scores.append(mse)\n",
    "        corr_scores.append(corr)\n",
    "        \n",
    "        #print(f\"Fold {fold+1} - MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    \n",
    "    return np.mean(mse_scores), np.mean(corr_scores)\n",
    "\n",
    "# Load and preprocess data\n",
    "densenet_df = pd.read_csv('densenet_hippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "pointnet_df = pd.read_csv('pointnet_hippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "random_forest_df = pd.read_csv('random_forest_pyradiomics_hippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "X = np.hstack((\n",
    "    densenet_df.iloc[:, 2:17].values,\n",
    "    pointnet_df.iloc[:, 2:17].values,\n",
    "    random_forest_df.iloc[:, 2:17].values\n",
    "))\n",
    "y = densenet_df['Actual'].values\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32]],\n",
    "    'dropout_rate': [0, 0.3, 0.5],\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "    'epochs': [1000],\n",
    "    'patience': [50],\n",
    "    'output_size': [1]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "for params in product(*param_grid.values()):\n",
    "    config = dict(zip(param_grid.keys(), params))\n",
    "    mse, corr = cross_validate_mlp(X, y, **config)\n",
    "    results.append((config, mse, corr))\n",
    "    # print(f\"Config: {config}\")\n",
    "    # print(f\"MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    # print(\"-\" * 50)\n",
    "\n",
    "# Sort results by MSE (ascending) and positive correlation (descending)\n",
    "results_by_mse = sorted(results, key=lambda x: x[1])\n",
    "results_by_corr = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "def print_top_5(sorted_results, metric_name):\n",
    "    print(f\"\\nTop 5 configurations by {metric_name}:\")\n",
    "    for i, (config, mse, corr) in enumerate(sorted_results[:5], 1):\n",
    "        print(f\"\\n{i}. Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        print(f\"   MSE: {mse:.4f}\")\n",
    "        print(f\"   Correlation: {corr:.4f}\")\n",
    "\n",
    "# Print top 5 by MSE\n",
    "print_top_5(results_by_mse, \"MSE\")\n",
    "\n",
    "# Print top 5 by Correlation\n",
    "print_top_5(results_by_corr, \"Correlation\")\n",
    "\n",
    "# Best configuration by MSE\n",
    "best_config_mse, best_mse, _ = results_by_mse[0]\n",
    "print(\"\\nBest configuration by MSE:\")\n",
    "for key, value in best_config_mse.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"MSE: {best_mse:.4f}\")\n",
    "\n",
    "# Best configuration by Correlation\n",
    "best_config_corr, _, best_corr = results_by_corr[0]\n",
    "print(\"\\nBest configuration by Correlation:\")\n",
    "for key, value in best_config_corr.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Correlation: {best_corr:.4f}\")\n",
    "\n",
    "# Diagnostic: Print predictions vs actual for best correlation model\n",
    "input_size = X.shape[1]\n",
    "best_model = MLPEnsemble(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_config_corr['hidden_sizes'],\n",
    "    output_size=best_config_corr['output_size'],\n",
    "    dropout_rate=best_config_corr['dropout_rate']\n",
    ")\n",
    "best_model = train_model(best_model, torch.FloatTensor(X), torch.FloatTensor(y), torch.FloatTensor(X), torch.FloatTensor(y), \n",
    "                         epochs=best_config_corr['epochs'], patience=best_config_corr['patience'], \n",
    "                         lr=best_config_corr['lr'], weight_decay=best_config_corr['weight_decay'], seed=GLOBAL_SEED)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(torch.FloatTensor(X)).squeeze().numpy()\n",
    "    \n",
    "# print(\"\\nSample of predictions vs actual:\")\n",
    "# for i in range(min(20, len(y))):\n",
    "#     print(f\"Actual: {y[i]:.4f}, Predicted: {predictions[i]:.4f}\")\n",
    "\n",
    "# # Correlation of individual input features with target\n",
    "# print(\"\\nCorrelation of individual features with target:\")\n",
    "# for i in range(X.shape[1]):\n",
    "#     corr, _ = pearsonr(X[:, i], y)\n",
    "#     print(f\"Feature {i+1}: {corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac9a3d5-6e2b-41ea-b690-00b0bcff3b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 configurations by MSE:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0929\n",
      "   Correlation: 0.1766\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0958\n",
      "   Correlation: 0.2266\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1027\n",
      "   Correlation: 0.1901\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [256]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1071\n",
      "   Correlation: 0.2641\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1079\n",
      "   Correlation: 0.1123\n",
      "\n",
      "Top 5 configurations by Absolute Correlation:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1754\n",
      "   Correlation: 0.2890\n",
      "   |Correlation|: 0.2890\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [256]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1071\n",
      "   Correlation: 0.2641\n",
      "   |Correlation|: 0.2641\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1121\n",
      "   Correlation: 0.2472\n",
      "   |Correlation|: 0.2472\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1095\n",
      "   Correlation: 0.2426\n",
      "   |Correlation|: 0.2426\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0958\n",
      "   Correlation: 0.2266\n",
      "   |Correlation|: 0.2266\n",
      "\n",
      "Best configuration by MSE:\n",
      "hidden_sizes: [128]\n",
      "dropout_rate: 0.5\n",
      "lr: 0.001\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "MSE: 0.0929\n",
      "\n",
      "Best configuration by Absolute Correlation:\n",
      "hidden_sizes: [128, 64, 32]\n",
      "dropout_rate: 0.5\n",
      "lr: 0.0005\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "Correlation: 0.2890\n",
      "|Correlation|: 0.2890\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a global seed\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "class MLPEnsemble(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(MLPEnsemble, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, patience, lr, weight_decay, seed):\n",
    "    set_seed(seed)  # Set seed for this training run\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).squeeze()\n",
    "    \n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "    predictions_np = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    \n",
    "    mse = mean_squared_error(y_np, predictions_np)\n",
    "    correlation, _ = pearsonr(y_np, predictions_np)\n",
    "    return mse, correlation\n",
    "\n",
    "def cross_validate_mlp(X, y, n_splits=5, **params):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    mse_scores = []\n",
    "    corr_scores = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    model_params = {\n",
    "        'input_size': X.shape[1],\n",
    "        'hidden_sizes': params['hidden_sizes'],\n",
    "        'output_size': params['output_size'],\n",
    "        'dropout_rate': params['dropout_rate']\n",
    "    }\n",
    "    train_params = {\n",
    "        'epochs': params['epochs'],\n",
    "        'patience': params['patience'],\n",
    "        'lr': params['lr'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'seed': GLOBAL_SEED\n",
    "    }\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        \n",
    "        model = MLPEnsemble(**model_params)\n",
    "        model = train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, **train_params)\n",
    "        \n",
    "        mse, corr = evaluate_model(model, X_val_tensor, y_val_tensor)\n",
    "        mse_scores.append(mse)\n",
    "        corr_scores.append(corr)\n",
    "        \n",
    "        #print(f\"Fold {fold+1} - MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    \n",
    "    return np.mean(mse_scores), np.mean(corr_scores)\n",
    "\n",
    "# Load and preprocess data\n",
    "densenet_df = pd.read_csv('densenet_hippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "pointnet_df = pd.read_csv('pointnet_hippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "random_forest_df = pd.read_csv('random_forest_pyradiomics_hippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "X = np.hstack((\n",
    "    densenet_df.iloc[:, 2:17].values,\n",
    "    pointnet_df.iloc[:, 2:17].values,\n",
    "    random_forest_df.iloc[:, 2:17].values\n",
    "))\n",
    "y = densenet_df['Actual'].values\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32]],\n",
    "    'dropout_rate': [0, 0.3, 0.5],\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "    'epochs': [1000],\n",
    "    'patience': [50],\n",
    "    'output_size': [1]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "for params in product(*param_grid.values()):\n",
    "    config = dict(zip(param_grid.keys(), params))\n",
    "    mse, corr = cross_validate_mlp(X, y, **config)\n",
    "    results.append((config, mse, corr))\n",
    "    #print(f\"Config: {config}\")\n",
    "    #print(f\"MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "# Sort results by MSE (ascending) and absolute correlation (descending)\n",
    "results_by_mse = sorted(results, key=lambda x: x[1])\n",
    "results_by_corr = sorted(results, key=lambda x: abs(x[2]), reverse=True)  # Modified to use absolute value\n",
    "\n",
    "def print_top_5(sorted_results, metric_name):\n",
    "    print(f\"\\nTop 5 configurations by {metric_name}:\")\n",
    "    for i, (config, mse, corr) in enumerate(sorted_results[:5], 1):\n",
    "        print(f\"\\n{i}. Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        print(f\"   MSE: {mse:.4f}\")\n",
    "        print(f\"   Correlation: {corr:.4f}\")\n",
    "        if metric_name == \"Absolute Correlation\":  # Show absolute value for clarity\n",
    "            print(f\"   |Correlation|: {abs(corr):.4f}\")\n",
    "\n",
    "# Print top 5 by MSE\n",
    "print_top_5(results_by_mse, \"MSE\")\n",
    "\n",
    "# Print top 5 by Absolute Correlation\n",
    "print_top_5(results_by_corr, \"Absolute Correlation\")\n",
    "\n",
    "# Best configuration by MSE\n",
    "best_config_mse, best_mse, _ = results_by_mse[0]\n",
    "print(\"\\nBest configuration by MSE:\")\n",
    "for key, value in best_config_mse.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"MSE: {best_mse:.4f}\")\n",
    "\n",
    "# Best configuration by Absolute Correlation\n",
    "best_config_corr, _, best_corr = results_by_corr[0]\n",
    "print(\"\\nBest configuration by Absolute Correlation:\")\n",
    "for key, value in best_config_corr.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Correlation: {best_corr:.4f}\")\n",
    "print(f\"|Correlation|: {abs(best_corr):.4f}\")\n",
    "\n",
    "# Diagnostic: Print predictions vs actual for best absolute correlation model\n",
    "input_size = X.shape[1]\n",
    "best_model = MLPEnsemble(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_config_corr['hidden_sizes'],\n",
    "    output_size=best_config_corr['output_size'],\n",
    "    dropout_rate=best_config_corr['dropout_rate']\n",
    ")\n",
    "best_model = train_model(best_model, torch.FloatTensor(X), torch.FloatTensor(y), torch.FloatTensor(X), torch.FloatTensor(y), \n",
    "                         epochs=best_config_corr['epochs'], patience=best_config_corr['patience'], \n",
    "                         lr=best_config_corr['lr'], weight_decay=best_config_corr['weight_decay'], seed=GLOBAL_SEED)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(torch.FloatTensor(X)).squeeze().numpy()\n",
    "    \n",
    "# print(\"\\nSample of predictions vs actual:\")\n",
    "# for i in range(min(20, len(y))):\n",
    "#     print(f\"Actual: {y[i]:.4f}, Predicted: {predictions[i]:.4f}\")\n",
    "\n",
    "# # Correlation of individual input features with target\n",
    "# print(\"\\nCorrelation of individual features with target:\")\n",
    "# for i in range(X.shape[1]):\n",
    "#     corr, _ = pearsonr(X[:, i], y)\n",
    "#     print(f\"Feature {i+1}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b69208-5b81-407d-87b0-47fc608f905c",
   "metadata": {},
   "source": [
    "## Thalamus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c00ab43-b57b-4945-928b-dca13f59e725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Methods:\n",
      "Simple Average Ensemble - MSE: 0.0668, Correlation: 0.2280\n",
      "Weighted Average Ensemble - MSE: 0.1196, Correlation: 0.3637\n",
      "Non-linear Weighted Average - MSE: 0.0930, Correlation: 0.1776\n",
      "Tuned Random Forest Meta-learner - MSE: 0.0767, Correlation: 0.0406\n",
      "Stacking Ensemble - MSE: 0.0744, Correlation: 0.1081\n",
      "\n",
      "Original Models:\n",
      "DenseNet - MSE: 0.0724, Correlation: 0.2414\n",
      "PointNet - MSE: 0.0760, Correlation: 0.1033\n",
      "Random Forest - MSE: 0.0691, Correlation: 0.2111\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr, randint\n",
    "from scipy import linalg\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess_data():\n",
    "    densenet_df = pd.read_csv('densenet_thalamus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "    pointnet_df = pd.read_csv('pointnet_thalamus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "    random_forest_df = pd.read_csv('random_forest_pyradiomics_thalamus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "    X = np.hstack((\n",
    "        densenet_df.iloc[:, 2:17].values,\n",
    "        pointnet_df.iloc[:, 2:17].values,\n",
    "        random_forest_df.iloc[:, 2:17].values\n",
    "    ))\n",
    "    y = densenet_df['Actual'].values\n",
    "\n",
    "    return X, y, densenet_df, pointnet_df, random_forest_df\n",
    "\n",
    "# Ensemble methods\n",
    "def simple_average_ensemble(X):\n",
    "    return np.mean(X, axis=1)\n",
    "\n",
    "def weighted_average_ensemble(X_train, y_train, X_test):\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    return lr_model.predict(X_test)\n",
    "\n",
    "def non_linear_weighted_average(X_train, y_train, X_test):\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    return mlp.predict(X_test)\n",
    "\n",
    "def tune_rf_meta_learner(X_train, y_train):\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(5, 50),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 10)\n",
    "    }\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n",
    "                                   n_iter=100, cv=5, random_state=42, n_jobs=-1)\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    return rf_random.best_estimator_\n",
    "\n",
    "def stacking_ensemble(X_train, y_train, X_test):\n",
    "    models = [\n",
    "        RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        SVR(kernel='rbf')\n",
    "    ]\n",
    "    \n",
    "    second_level_train = np.column_stack([\n",
    "        cross_val_predict(model, X_train, y_train, cv=5) \n",
    "        for model in models\n",
    "    ])\n",
    "    second_level_test = np.column_stack([\n",
    "        model.fit(X_train, y_train).predict(X_test) \n",
    "        for model in models\n",
    "    ])\n",
    "    \n",
    "    # Use Ridge regression as the meta-learner\n",
    "    alpha = 1.0\n",
    "    n_samples, n_features = second_level_train.shape\n",
    "    \n",
    "    # Add a small value to the diagonal for numerical stability\n",
    "    A = np.dot(second_level_train.T, second_level_train)\n",
    "    A.flat[::n_features + 1] += alpha + 1e-10\n",
    "    \n",
    "    b = np.dot(second_level_train.T, y_train)\n",
    "    \n",
    "    # Solve the linear system\n",
    "    try:\n",
    "        coef = linalg.solve(A, b, assume_a='pos')\n",
    "    except TypeError:\n",
    "        # For older versions of SciPy\n",
    "        coef = linalg.solve(A, b, sym_pos=True)\n",
    "    \n",
    "    # Make predictions\n",
    "    return np.dot(second_level_test, coef)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    correlation, _ = pearsonr(y_true, y_pred)\n",
    "    return mse, correlation\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    X, y, densenet_df, pointnet_df, random_forest_df = load_and_preprocess_data()\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    simple_avg_scores = []\n",
    "    weighted_avg_scores = []\n",
    "    non_linear_weighted_scores = []\n",
    "    rf_meta_scores = []\n",
    "    stacking_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Simple average\n",
    "        simple_avg_pred = simple_average_ensemble(X_test)\n",
    "        simple_avg_scores.append(evaluate_model(y_test, simple_avg_pred))\n",
    "        \n",
    "        # Weighted average\n",
    "        weighted_avg_pred = weighted_average_ensemble(X_train, y_train, X_test)\n",
    "        weighted_avg_scores.append(evaluate_model(y_test, weighted_avg_pred))\n",
    "        \n",
    "        # Non-linear weighted average\n",
    "        non_linear_weighted_pred = non_linear_weighted_average(X_train, y_train, X_test)\n",
    "        non_linear_weighted_scores.append(evaluate_model(y_test, non_linear_weighted_pred))\n",
    "        \n",
    "        # Random Forest meta-learner (with hyperparameter tuning)\n",
    "        rf_model = tune_rf_meta_learner(X_train, y_train)\n",
    "        rf_pred = rf_model.predict(X_test)\n",
    "        rf_meta_scores.append(evaluate_model(y_test, rf_pred))\n",
    "        \n",
    "        # Stacking ensemble\n",
    "        stacking_pred = stacking_ensemble(X_train, y_train, X_test)\n",
    "        stacking_scores.append(evaluate_model(y_test, stacking_pred))\n",
    "\n",
    "    # Calculate and print average scores\n",
    "    print(\"Ensemble Methods:\")\n",
    "    print(f\"Simple Average Ensemble - MSE: {np.mean([s[0] for s in simple_avg_scores]):.4f}, Correlation: {np.mean([s[1] for s in simple_avg_scores]):.4f}\")\n",
    "    print(f\"Weighted Average Ensemble - MSE: {np.mean([s[0] for s in weighted_avg_scores]):.4f}, Correlation: {np.mean([s[1] for s in weighted_avg_scores]):.4f}\")\n",
    "    print(f\"Non-linear Weighted Average - MSE: {np.mean([s[0] for s in non_linear_weighted_scores]):.4f}, Correlation: {np.mean([s[1] for s in non_linear_weighted_scores]):.4f}\")\n",
    "    print(f\"Tuned Random Forest Meta-learner - MSE: {np.mean([s[0] for s in rf_meta_scores]):.4f}, Correlation: {np.mean([s[1] for s in rf_meta_scores]):.4f}\")\n",
    "    print(f\"Stacking Ensemble - MSE: {np.mean([s[0] for s in stacking_scores]):.4f}, Correlation: {np.mean([s[1] for s in stacking_scores]):.4f}\")\n",
    "\n",
    "    # Evaluate original models\n",
    "    print(\"\\nOriginal Models:\")\n",
    "    for model_name, df in zip(['DenseNet', 'PointNet', 'Random Forest'], \n",
    "                              [densenet_df, pointnet_df, random_forest_df]):\n",
    "        avg_pred = df['Average_Prediction'].values\n",
    "        mse, correlation = evaluate_model(df['Actual'], avg_pred)\n",
    "        print(f\"{model_name} - MSE: {mse:.4f}, Correlation: {correlation:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bdcad-2924-476e-a3b9-5dfd68e2cc4a",
   "metadata": {},
   "source": [
    "### MLP with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f382dcc-87cd-4839-9dc2-58f7e23ce8bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 configurations by MSE:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0787\n",
      "   Correlation: 0.2335\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [256]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0849\n",
      "   Correlation: 0.2455\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [256]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0853\n",
      "   Correlation: 0.3109\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0860\n",
      "   Correlation: 0.2075\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0876\n",
      "   Correlation: 0.2612\n",
      "\n",
      "Top 5 configurations by Correlation:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0971\n",
      "   Correlation: 0.3860\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1200\n",
      "   Correlation: 0.3132\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [256]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0853\n",
      "   Correlation: 0.3109\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1506\n",
      "   Correlation: 0.3087\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0\n",
      "   lr: 0.0005\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1314\n",
      "   Correlation: 0.2959\n",
      "\n",
      "Best configuration by MSE:\n",
      "hidden_sizes: [64]\n",
      "dropout_rate: 0.5\n",
      "lr: 0.0005\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "MSE: 0.0787\n",
      "\n",
      "Best configuration by Correlation:\n",
      "hidden_sizes: [128, 64]\n",
      "dropout_rate: 0.5\n",
      "lr: 0.0005\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "Correlation: 0.3860\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a global seed\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "class MLPEnsemble(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(MLPEnsemble, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, patience, lr, weight_decay, seed):\n",
    "    set_seed(seed)  # Set seed for this training run\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).squeeze()\n",
    "    \n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "    predictions_np = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    \n",
    "    mse = mean_squared_error(y_np, predictions_np)\n",
    "    correlation, _ = pearsonr(y_np, predictions_np)\n",
    "    return mse, correlation\n",
    "\n",
    "def cross_validate_mlp(X, y, n_splits=5, **params):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    mse_scores = []\n",
    "    corr_scores = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    model_params = {\n",
    "        'input_size': X.shape[1],\n",
    "        'hidden_sizes': params['hidden_sizes'],\n",
    "        'output_size': params['output_size'],\n",
    "        'dropout_rate': params['dropout_rate']\n",
    "    }\n",
    "    train_params = {\n",
    "        'epochs': params['epochs'],\n",
    "        'patience': params['patience'],\n",
    "        'lr': params['lr'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'seed': GLOBAL_SEED\n",
    "    }\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        \n",
    "        model = MLPEnsemble(**model_params)\n",
    "        model = train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, **train_params)\n",
    "        \n",
    "        mse, corr = evaluate_model(model, X_val_tensor, y_val_tensor)\n",
    "        mse_scores.append(mse)\n",
    "        corr_scores.append(corr)\n",
    "        \n",
    "        #print(f\"Fold {fold+1} - MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    \n",
    "    return np.mean(mse_scores), np.mean(corr_scores)\n",
    "\n",
    "# Load and preprocess data\n",
    "densenet_df = pd.read_csv('densenet_thalamus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "pointnet_df = pd.read_csv('pointnet_thalamus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "random_forest_df = pd.read_csv('random_forest_pyradiomics_thalamus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "X = np.hstack((\n",
    "    densenet_df.iloc[:, 2:17].values,\n",
    "    pointnet_df.iloc[:, 2:17].values,\n",
    "    random_forest_df.iloc[:, 2:17].values\n",
    "))\n",
    "y = densenet_df['Actual'].values\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32]],\n",
    "    'dropout_rate': [0, 0.3, 0.5],\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "    'epochs': [1000],\n",
    "    'patience': [50],\n",
    "    'output_size': [1]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "for params in product(*param_grid.values()):\n",
    "    config = dict(zip(param_grid.keys(), params))\n",
    "    mse, corr = cross_validate_mlp(X, y, **config)\n",
    "    results.append((config, mse, corr))\n",
    "    #print(f\"Config: {config}\")\n",
    "    #print(f\"MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "# Sort results by MSE (ascending) and positive correlation (descending)\n",
    "results_by_mse = sorted(results, key=lambda x: x[1])\n",
    "results_by_corr = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "def print_top_5(sorted_results, metric_name):\n",
    "    print(f\"\\nTop 5 configurations by {metric_name}:\")\n",
    "    for i, (config, mse, corr) in enumerate(sorted_results[:5], 1):\n",
    "        print(f\"\\n{i}. Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        print(f\"   MSE: {mse:.4f}\")\n",
    "        print(f\"   Correlation: {corr:.4f}\")\n",
    "\n",
    "# Print top 5 by MSE\n",
    "print_top_5(results_by_mse, \"MSE\")\n",
    "\n",
    "# Print top 5 by Correlation\n",
    "print_top_5(results_by_corr, \"Correlation\")\n",
    "\n",
    "# Best configuration by MSE\n",
    "best_config_mse, best_mse, _ = results_by_mse[0]\n",
    "print(\"\\nBest configuration by MSE:\")\n",
    "for key, value in best_config_mse.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"MSE: {best_mse:.4f}\")\n",
    "\n",
    "# Best configuration by Correlation\n",
    "best_config_corr, _, best_corr = results_by_corr[0]\n",
    "print(\"\\nBest configuration by Correlation:\")\n",
    "for key, value in best_config_corr.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Correlation: {best_corr:.4f}\")\n",
    "\n",
    "# Diagnostic: Print predictions vs actual for best correlation model\n",
    "input_size = X.shape[1]\n",
    "best_model = MLPEnsemble(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_config_corr['hidden_sizes'],\n",
    "    output_size=best_config_corr['output_size'],\n",
    "    dropout_rate=best_config_corr['dropout_rate']\n",
    ")\n",
    "best_model = train_model(best_model, torch.FloatTensor(X), torch.FloatTensor(y), torch.FloatTensor(X), torch.FloatTensor(y), \n",
    "                         epochs=best_config_corr['epochs'], patience=best_config_corr['patience'], \n",
    "                         lr=best_config_corr['lr'], weight_decay=best_config_corr['weight_decay'], seed=GLOBAL_SEED)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(torch.FloatTensor(X)).squeeze().numpy()\n",
    "    \n",
    "# print(\"\\nSample of predictions vs actual:\")\n",
    "# for i in range(min(20, len(y))):\n",
    "#     print(f\"Actual: {y[i]:.4f}, Predicted: {predictions[i]:.4f}\")\n",
    "\n",
    "# # Correlation of individual input features with target\n",
    "# print(\"\\nCorrelation of individual features with target:\")\n",
    "# for i in range(X.shape[1]):\n",
    "#     corr, _ = pearsonr(X[:, i], y)\n",
    "#     print(f\"Feature {i+1}: {corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "447ec4dd-bf3b-4d94-b47f-b32e2d049395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 configurations by MSE:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0787\n",
      "   Correlation: 0.2335\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [256]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0849\n",
      "   Correlation: 0.2455\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [256]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0853\n",
      "   Correlation: 0.3109\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0860\n",
      "   Correlation: 0.2075\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0876\n",
      "   Correlation: 0.2612\n",
      "\n",
      "Top 5 configurations by Absolute Correlation:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0971\n",
      "   Correlation: 0.3860\n",
      "   |Correlation|: 0.3860\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1200\n",
      "   Correlation: 0.3132\n",
      "   |Correlation|: 0.3132\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [256]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0853\n",
      "   Correlation: 0.3109\n",
      "   |Correlation|: 0.3109\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1506\n",
      "   Correlation: 0.3087\n",
      "   |Correlation|: 0.3087\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0\n",
      "   lr: 0.0005\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1314\n",
      "   Correlation: 0.2959\n",
      "   |Correlation|: 0.2959\n",
      "\n",
      "Best configuration by MSE:\n",
      "hidden_sizes: [64]\n",
      "dropout_rate: 0.5\n",
      "lr: 0.0005\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "MSE: 0.0787\n",
      "\n",
      "Best configuration by Absolute Correlation:\n",
      "hidden_sizes: [128, 64]\n",
      "dropout_rate: 0.5\n",
      "lr: 0.0005\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "Correlation: 0.3860\n",
      "|Correlation|: 0.3860\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a global seed\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "class MLPEnsemble(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(MLPEnsemble, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, patience, lr, weight_decay, seed):\n",
    "    set_seed(seed)  # Set seed for this training run\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).squeeze()\n",
    "    \n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "    predictions_np = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    \n",
    "    mse = mean_squared_error(y_np, predictions_np)\n",
    "    correlation, _ = pearsonr(y_np, predictions_np)\n",
    "    return mse, correlation\n",
    "\n",
    "def cross_validate_mlp(X, y, n_splits=5, **params):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    mse_scores = []\n",
    "    corr_scores = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    model_params = {\n",
    "        'input_size': X.shape[1],\n",
    "        'hidden_sizes': params['hidden_sizes'],\n",
    "        'output_size': params['output_size'],\n",
    "        'dropout_rate': params['dropout_rate']\n",
    "    }\n",
    "    train_params = {\n",
    "        'epochs': params['epochs'],\n",
    "        'patience': params['patience'],\n",
    "        'lr': params['lr'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'seed': GLOBAL_SEED\n",
    "    }\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        \n",
    "        model = MLPEnsemble(**model_params)\n",
    "        model = train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, **train_params)\n",
    "        \n",
    "        mse, corr = evaluate_model(model, X_val_tensor, y_val_tensor)\n",
    "        mse_scores.append(mse)\n",
    "        corr_scores.append(corr)\n",
    "        \n",
    "        #print(f\"Fold {fold+1} - MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    \n",
    "    return np.mean(mse_scores), np.mean(corr_scores)\n",
    "\n",
    "# Load and preprocess data\n",
    "densenet_df = pd.read_csv('densenet_thalamus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "pointnet_df = pd.read_csv('pointnet_thalamus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "random_forest_df = pd.read_csv('random_forest_pyradiomics_thalamus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "X = np.hstack((\n",
    "    densenet_df.iloc[:, 2:17].values,\n",
    "    pointnet_df.iloc[:, 2:17].values,\n",
    "    random_forest_df.iloc[:, 2:17].values\n",
    "))\n",
    "y = densenet_df['Actual'].values\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32]],\n",
    "    'dropout_rate': [0, 0.3, 0.5],\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "    'epochs': [1000],\n",
    "    'patience': [50],\n",
    "    'output_size': [1]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "for params in product(*param_grid.values()):\n",
    "    config = dict(zip(param_grid.keys(), params))\n",
    "    mse, corr = cross_validate_mlp(X, y, **config)\n",
    "    results.append((config, mse, corr))\n",
    "    #print(f\"Config: {config}\")\n",
    "    #print(f\"MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "# Sort results by MSE (ascending) and absolute correlation (descending)\n",
    "results_by_mse = sorted(results, key=lambda x: x[1])\n",
    "results_by_corr = sorted(results, key=lambda x: abs(x[2]), reverse=True)  # Modified to use absolute value\n",
    "\n",
    "def print_top_5(sorted_results, metric_name):\n",
    "    print(f\"\\nTop 5 configurations by {metric_name}:\")\n",
    "    for i, (config, mse, corr) in enumerate(sorted_results[:5], 1):\n",
    "        print(f\"\\n{i}. Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        print(f\"   MSE: {mse:.4f}\")\n",
    "        print(f\"   Correlation: {corr:.4f}\")\n",
    "        if metric_name == \"Absolute Correlation\":  # Show absolute value for clarity\n",
    "            print(f\"   |Correlation|: {abs(corr):.4f}\")\n",
    "\n",
    "# Print top 5 by MSE\n",
    "print_top_5(results_by_mse, \"MSE\")\n",
    "\n",
    "# Print top 5 by Absolute Correlation\n",
    "print_top_5(results_by_corr, \"Absolute Correlation\")\n",
    "\n",
    "# Best configuration by MSE\n",
    "best_config_mse, best_mse, _ = results_by_mse[0]\n",
    "print(\"\\nBest configuration by MSE:\")\n",
    "for key, value in best_config_mse.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"MSE: {best_mse:.4f}\")\n",
    "\n",
    "# Best configuration by Absolute Correlation\n",
    "best_config_corr, _, best_corr = results_by_corr[0]\n",
    "print(\"\\nBest configuration by Absolute Correlation:\")\n",
    "for key, value in best_config_corr.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Correlation: {best_corr:.4f}\")\n",
    "print(f\"|Correlation|: {abs(best_corr):.4f}\")\n",
    "\n",
    "# Diagnostic: Print predictions vs actual for best absolute correlation model\n",
    "input_size = X.shape[1]\n",
    "best_model = MLPEnsemble(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_config_corr['hidden_sizes'],\n",
    "    output_size=best_config_corr['output_size'],\n",
    "    dropout_rate=best_config_corr['dropout_rate']\n",
    ")\n",
    "best_model = train_model(best_model, torch.FloatTensor(X), torch.FloatTensor(y), torch.FloatTensor(X), torch.FloatTensor(y), \n",
    "                         epochs=best_config_corr['epochs'], patience=best_config_corr['patience'], \n",
    "                         lr=best_config_corr['lr'], weight_decay=best_config_corr['weight_decay'], seed=GLOBAL_SEED)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(torch.FloatTensor(X)).squeeze().numpy()\n",
    "    \n",
    "# print(\"\\nSample of predictions vs actual:\")\n",
    "# for i in range(min(20, len(y))):\n",
    "#     print(f\"Actual: {y[i]:.4f}, Predicted: {predictions[i]:.4f}\")\n",
    "\n",
    "# # Correlation of individual input features with target\n",
    "# print(\"\\nCorrelation of individual features with target:\")\n",
    "# for i in range(X.shape[1]):\n",
    "#     corr, _ = pearsonr(X[:, i], y)\n",
    "#     print(f\"Feature {i+1}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61317d11-a4fc-4840-a8bd-d72e5866b95f",
   "metadata": {},
   "source": [
    "## Whole brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4af157-d45d-4a00-b671-3b574645e535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Methods:\n",
      "Simple Average Ensemble - MSE: 0.0711, Correlation: 0.1928\n",
      "Weighted Average Ensemble - MSE: 0.2491, Correlation: 0.0845\n",
      "Non-linear Weighted Average - MSE: 0.1149, Correlation: 0.1087\n",
      "Tuned Random Forest Meta-learner - MSE: 0.0799, Correlation: 0.0349\n",
      "Stacking Ensemble - MSE: 0.0833, Correlation: 0.0256\n",
      "\n",
      "Original Models:\n",
      "DenseNet - MSE: 0.0835, Correlation: 0.1025\n",
      "PointNet - MSE: 0.0749, Correlation: 0.1843\n",
      "Random Forest - MSE: 0.0678, Correlation: 0.2474\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr, randint\n",
    "from scipy import linalg\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess_data():\n",
    "    densenet_df = pd.read_csv('densenet_wholebrain_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "    pointnet_df = pd.read_csv('pointnet_wholebrain_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "    random_forest_df = pd.read_csv('random_forest_pyradiomics_wholebrain_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "    X = np.hstack((\n",
    "        densenet_df.iloc[:, 2:17].values,\n",
    "        pointnet_df.iloc[:, 2:17].values,\n",
    "        random_forest_df.iloc[:, 2:17].values\n",
    "    ))\n",
    "    y = densenet_df['Actual'].values\n",
    "\n",
    "    return X, y, densenet_df, pointnet_df, random_forest_df\n",
    "\n",
    "# Ensemble methods\n",
    "def simple_average_ensemble(X):\n",
    "    return np.mean(X, axis=1)\n",
    "\n",
    "def weighted_average_ensemble(X_train, y_train, X_test):\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    return lr_model.predict(X_test)\n",
    "\n",
    "def non_linear_weighted_average(X_train, y_train, X_test):\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    return mlp.predict(X_test)\n",
    "\n",
    "def tune_rf_meta_learner(X_train, y_train):\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(5, 50),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 10)\n",
    "    }\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n",
    "                                   n_iter=100, cv=5, random_state=42, n_jobs=-1)\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    return rf_random.best_estimator_\n",
    "\n",
    "def stacking_ensemble(X_train, y_train, X_test):\n",
    "    models = [\n",
    "        RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        SVR(kernel='rbf')\n",
    "    ]\n",
    "    \n",
    "    second_level_train = np.column_stack([\n",
    "        cross_val_predict(model, X_train, y_train, cv=5) \n",
    "        for model in models\n",
    "    ])\n",
    "    second_level_test = np.column_stack([\n",
    "        model.fit(X_train, y_train).predict(X_test) \n",
    "        for model in models\n",
    "    ])\n",
    "    \n",
    "    # Use Ridge regression as the meta-learner\n",
    "    alpha = 1.0\n",
    "    n_samples, n_features = second_level_train.shape\n",
    "    \n",
    "    # Add a small value to the diagonal for numerical stability\n",
    "    A = np.dot(second_level_train.T, second_level_train)\n",
    "    A.flat[::n_features + 1] += alpha + 1e-10\n",
    "    \n",
    "    b = np.dot(second_level_train.T, y_train)\n",
    "    \n",
    "    # Solve the linear system\n",
    "    try:\n",
    "        coef = linalg.solve(A, b, assume_a='pos')\n",
    "    except TypeError:\n",
    "        # For older versions of SciPy\n",
    "        coef = linalg.solve(A, b, sym_pos=True)\n",
    "    \n",
    "    # Make predictions\n",
    "    return np.dot(second_level_test, coef)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    correlation, _ = pearsonr(y_true, y_pred)\n",
    "    return mse, correlation\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    X, y, densenet_df, pointnet_df, random_forest_df = load_and_preprocess_data()\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    simple_avg_scores = []\n",
    "    weighted_avg_scores = []\n",
    "    non_linear_weighted_scores = []\n",
    "    rf_meta_scores = []\n",
    "    stacking_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Simple average\n",
    "        simple_avg_pred = simple_average_ensemble(X_test)\n",
    "        simple_avg_scores.append(evaluate_model(y_test, simple_avg_pred))\n",
    "        \n",
    "        # Weighted average\n",
    "        weighted_avg_pred = weighted_average_ensemble(X_train, y_train, X_test)\n",
    "        weighted_avg_scores.append(evaluate_model(y_test, weighted_avg_pred))\n",
    "        \n",
    "        # Non-linear weighted average\n",
    "        non_linear_weighted_pred = non_linear_weighted_average(X_train, y_train, X_test)\n",
    "        non_linear_weighted_scores.append(evaluate_model(y_test, non_linear_weighted_pred))\n",
    "        \n",
    "        # Random Forest meta-learner (with hyperparameter tuning)\n",
    "        rf_model = tune_rf_meta_learner(X_train, y_train)\n",
    "        rf_pred = rf_model.predict(X_test)\n",
    "        rf_meta_scores.append(evaluate_model(y_test, rf_pred))\n",
    "        \n",
    "        # Stacking ensemble\n",
    "        stacking_pred = stacking_ensemble(X_train, y_train, X_test)\n",
    "        stacking_scores.append(evaluate_model(y_test, stacking_pred))\n",
    "\n",
    "    # Calculate and print average scores\n",
    "    print(\"Ensemble Methods:\")\n",
    "    print(f\"Simple Average Ensemble - MSE: {np.mean([s[0] for s in simple_avg_scores]):.4f}, Correlation: {np.mean([s[1] for s in simple_avg_scores]):.4f}\")\n",
    "    print(f\"Weighted Average Ensemble - MSE: {np.mean([s[0] for s in weighted_avg_scores]):.4f}, Correlation: {np.mean([s[1] for s in weighted_avg_scores]):.4f}\")\n",
    "    print(f\"Non-linear Weighted Average - MSE: {np.mean([s[0] for s in non_linear_weighted_scores]):.4f}, Correlation: {np.mean([s[1] for s in non_linear_weighted_scores]):.4f}\")\n",
    "    print(f\"Tuned Random Forest Meta-learner - MSE: {np.mean([s[0] for s in rf_meta_scores]):.4f}, Correlation: {np.mean([s[1] for s in rf_meta_scores]):.4f}\")\n",
    "    print(f\"Stacking Ensemble - MSE: {np.mean([s[0] for s in stacking_scores]):.4f}, Correlation: {np.mean([s[1] for s in stacking_scores]):.4f}\")\n",
    "\n",
    "    # Evaluate original models\n",
    "    print(\"\\nOriginal Models:\")\n",
    "    for model_name, df in zip(['DenseNet', 'PointNet', 'Random Forest'], \n",
    "                              [densenet_df, pointnet_df, random_forest_df]):\n",
    "        avg_pred = df['Average_Prediction'].values\n",
    "        mse, correlation = evaluate_model(df['Actual'], avg_pred)\n",
    "        print(f\"{model_name} - MSE: {mse:.4f}, Correlation: {correlation:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78602d7-71f2-4318-8869-8e9122e66736",
   "metadata": {},
   "source": [
    "### MLP with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "578b79f0-0924-4a78-a970-e5cda14e5a7c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 configurations by MSE:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0897\n",
      "   Correlation: 0.2398\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0947\n",
      "   Correlation: 0.2438\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0957\n",
      "   Correlation: 0.1730\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0981\n",
      "   Correlation: 0.2876\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1007\n",
      "   Correlation: 0.1699\n",
      "\n",
      "Top 5 configurations by Correlation:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1234\n",
      "   Correlation: 0.3161\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0981\n",
      "   Correlation: 0.2876\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0\n",
      "   lr: 0.0001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.2087\n",
      "   Correlation: 0.2731\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0\n",
      "   lr: 0.0001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.2087\n",
      "   Correlation: 0.2731\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1139\n",
      "   Correlation: 0.2578\n",
      "\n",
      "Best configuration by MSE:\n",
      "hidden_sizes: [128, 64, 32]\n",
      "dropout_rate: 0.3\n",
      "lr: 0.001\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "MSE: 0.0897\n",
      "\n",
      "Best configuration by Correlation:\n",
      "hidden_sizes: [64, 32]\n",
      "dropout_rate: 0\n",
      "lr: 0.001\n",
      "weight_decay: 0.0001\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "Correlation: 0.3161\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a global seed\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "class MLPEnsemble(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(MLPEnsemble, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, patience, lr, weight_decay, seed):\n",
    "    set_seed(seed)  # Set seed for this training run\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).squeeze()\n",
    "    \n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "    predictions_np = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    \n",
    "    mse = mean_squared_error(y_np, predictions_np)\n",
    "    correlation, _ = pearsonr(y_np, predictions_np)\n",
    "    return mse, correlation\n",
    "\n",
    "def cross_validate_mlp(X, y, n_splits=5, **params):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    mse_scores = []\n",
    "    corr_scores = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    model_params = {\n",
    "        'input_size': X.shape[1],\n",
    "        'hidden_sizes': params['hidden_sizes'],\n",
    "        'output_size': params['output_size'],\n",
    "        'dropout_rate': params['dropout_rate']\n",
    "    }\n",
    "    train_params = {\n",
    "        'epochs': params['epochs'],\n",
    "        'patience': params['patience'],\n",
    "        'lr': params['lr'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'seed': GLOBAL_SEED\n",
    "    }\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        \n",
    "        model = MLPEnsemble(**model_params)\n",
    "        model = train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, **train_params)\n",
    "        \n",
    "        mse, corr = evaluate_model(model, X_val_tensor, y_val_tensor)\n",
    "        mse_scores.append(mse)\n",
    "        corr_scores.append(corr)\n",
    "        \n",
    "        #print(f\"Fold {fold+1} - MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    \n",
    "    return np.mean(mse_scores), np.mean(corr_scores)\n",
    "\n",
    "# Load and preprocess data\n",
    "densenet_df = pd.read_csv('densenet_wholebrain_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "pointnet_df = pd.read_csv('pointnet_wholebrain_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "random_forest_df = pd.read_csv('random_forest_pyradiomics_wholebrain_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "X = np.hstack((\n",
    "    densenet_df.iloc[:, 2:17].values,\n",
    "    pointnet_df.iloc[:, 2:17].values,\n",
    "    random_forest_df.iloc[:, 2:17].values\n",
    "))\n",
    "y = densenet_df['Actual'].values\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32]],\n",
    "    'dropout_rate': [0, 0.3, 0.5],\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "    'epochs': [1000],\n",
    "    'patience': [50],\n",
    "    'output_size': [1]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "for params in product(*param_grid.values()):\n",
    "    config = dict(zip(param_grid.keys(), params))\n",
    "    mse, corr = cross_validate_mlp(X, y, **config)\n",
    "    results.append((config, mse, corr))\n",
    "    #print(f\"Config: {config}\")\n",
    "    #print(f\"MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "# Sort results by MSE (ascending) and positive correlation (descending)\n",
    "results_by_mse = sorted(results, key=lambda x: x[1])\n",
    "results_by_corr = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "def print_top_5(sorted_results, metric_name):\n",
    "    print(f\"\\nTop 5 configurations by {metric_name}:\")\n",
    "    for i, (config, mse, corr) in enumerate(sorted_results[:5], 1):\n",
    "        print(f\"\\n{i}. Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        print(f\"   MSE: {mse:.4f}\")\n",
    "        print(f\"   Correlation: {corr:.4f}\")\n",
    "\n",
    "# Print top 5 by MSE\n",
    "print_top_5(results_by_mse, \"MSE\")\n",
    "\n",
    "# Print top 5 by Correlation\n",
    "print_top_5(results_by_corr, \"Correlation\")\n",
    "\n",
    "# Best configuration by MSE\n",
    "best_config_mse, best_mse, _ = results_by_mse[0]\n",
    "print(\"\\nBest configuration by MSE:\")\n",
    "for key, value in best_config_mse.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"MSE: {best_mse:.4f}\")\n",
    "\n",
    "# Best configuration by Correlation\n",
    "best_config_corr, _, best_corr = results_by_corr[0]\n",
    "print(\"\\nBest configuration by Correlation:\")\n",
    "for key, value in best_config_corr.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Correlation: {best_corr:.4f}\")\n",
    "\n",
    "# Diagnostic: Print predictions vs actual for best correlation model\n",
    "input_size = X.shape[1]\n",
    "best_model = MLPEnsemble(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_config_corr['hidden_sizes'],\n",
    "    output_size=best_config_corr['output_size'],\n",
    "    dropout_rate=best_config_corr['dropout_rate']\n",
    ")\n",
    "best_model = train_model(best_model, torch.FloatTensor(X), torch.FloatTensor(y), torch.FloatTensor(X), torch.FloatTensor(y), \n",
    "                         epochs=best_config_corr['epochs'], patience=best_config_corr['patience'], \n",
    "                         lr=best_config_corr['lr'], weight_decay=best_config_corr['weight_decay'], seed=GLOBAL_SEED)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(torch.FloatTensor(X)).squeeze().numpy()\n",
    "    \n",
    "# print(\"\\nSample of predictions vs actual:\")\n",
    "# for i in range(min(20, len(y))):\n",
    "#     print(f\"Actual: {y[i]:.4f}, Predicted: {predictions[i]:.4f}\")\n",
    "\n",
    "# # Correlation of individual input features with target\n",
    "# print(\"\\nCorrelation of individual features with target:\")\n",
    "# for i in range(X.shape[1]):\n",
    "#     corr, _ = pearsonr(X[:, i], y)\n",
    "#     print(f\"Feature {i+1}: {corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cc816fe-2200-4470-8a69-c7dbd8225dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 configurations by MSE:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0897\n",
      "   Correlation: 0.2398\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0947\n",
      "   Correlation: 0.2438\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0957\n",
      "   Correlation: 0.1730\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0981\n",
      "   Correlation: 0.2876\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1007\n",
      "   Correlation: 0.1699\n",
      "\n",
      "Top 5 configurations by Absolute Correlation:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1234\n",
      "   Correlation: 0.3161\n",
      "   |Correlation|: 0.3161\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0981\n",
      "   Correlation: 0.2876\n",
      "   |Correlation|: 0.2876\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0\n",
      "   lr: 0.0001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.2087\n",
      "   Correlation: 0.2731\n",
      "   |Correlation|: 0.2731\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0\n",
      "   lr: 0.0001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.2087\n",
      "   Correlation: 0.2731\n",
      "   |Correlation|: 0.2731\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1139\n",
      "   Correlation: 0.2578\n",
      "   |Correlation|: 0.2578\n",
      "\n",
      "Best configuration by MSE:\n",
      "hidden_sizes: [128, 64, 32]\n",
      "dropout_rate: 0.3\n",
      "lr: 0.001\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "MSE: 0.0897\n",
      "\n",
      "Best configuration by Absolute Correlation:\n",
      "hidden_sizes: [64, 32]\n",
      "dropout_rate: 0\n",
      "lr: 0.001\n",
      "weight_decay: 0.0001\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "Correlation: 0.3161\n",
      "|Correlation|: 0.3161\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a global seed\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "class MLPEnsemble(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(MLPEnsemble, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, patience, lr, weight_decay, seed):\n",
    "    set_seed(seed)  # Set seed for this training run\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).squeeze()\n",
    "    \n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "    predictions_np = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    \n",
    "    mse = mean_squared_error(y_np, predictions_np)\n",
    "    correlation, _ = pearsonr(y_np, predictions_np)\n",
    "    return mse, correlation\n",
    "\n",
    "def cross_validate_mlp(X, y, n_splits=5, **params):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    mse_scores = []\n",
    "    corr_scores = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    model_params = {\n",
    "        'input_size': X.shape[1],\n",
    "        'hidden_sizes': params['hidden_sizes'],\n",
    "        'output_size': params['output_size'],\n",
    "        'dropout_rate': params['dropout_rate']\n",
    "    }\n",
    "    train_params = {\n",
    "        'epochs': params['epochs'],\n",
    "        'patience': params['patience'],\n",
    "        'lr': params['lr'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'seed': GLOBAL_SEED\n",
    "    }\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        \n",
    "        model = MLPEnsemble(**model_params)\n",
    "        model = train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, **train_params)\n",
    "        \n",
    "        mse, corr = evaluate_model(model, X_val_tensor, y_val_tensor)\n",
    "        mse_scores.append(mse)\n",
    "        corr_scores.append(corr)\n",
    "        \n",
    "        #print(f\"Fold {fold+1} - MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    \n",
    "    return np.mean(mse_scores), np.mean(corr_scores)\n",
    "\n",
    "# Load and preprocess data\n",
    "densenet_df = pd.read_csv('densenet_wholebrain_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "pointnet_df = pd.read_csv('pointnet_wholebrain_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "random_forest_df = pd.read_csv('random_forest_pyradiomics_wholebrain_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "X = np.hstack((\n",
    "    densenet_df.iloc[:, 2:17].values,\n",
    "    pointnet_df.iloc[:, 2:17].values,\n",
    "    random_forest_df.iloc[:, 2:17].values\n",
    "))\n",
    "y = densenet_df['Actual'].values\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32]],\n",
    "    'dropout_rate': [0, 0.3, 0.5],\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "    'epochs': [1000],\n",
    "    'patience': [50],\n",
    "    'output_size': [1]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "for params in product(*param_grid.values()):\n",
    "    config = dict(zip(param_grid.keys(), params))\n",
    "    mse, corr = cross_validate_mlp(X, y, **config)\n",
    "    results.append((config, mse, corr))\n",
    "    #print(f\"Config: {config}\")\n",
    "    #print(f\"MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "# Sort results by MSE (ascending) and absolute correlation (descending)\n",
    "results_by_mse = sorted(results, key=lambda x: x[1])\n",
    "results_by_corr = sorted(results, key=lambda x: abs(x[2]), reverse=True)  # Modified to use absolute value\n",
    "\n",
    "def print_top_5(sorted_results, metric_name):\n",
    "    print(f\"\\nTop 5 configurations by {metric_name}:\")\n",
    "    for i, (config, mse, corr) in enumerate(sorted_results[:5], 1):\n",
    "        print(f\"\\n{i}. Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        print(f\"   MSE: {mse:.4f}\")\n",
    "        print(f\"   Correlation: {corr:.4f}\")\n",
    "        if metric_name == \"Absolute Correlation\":  # Show absolute value for clarity\n",
    "            print(f\"   |Correlation|: {abs(corr):.4f}\")\n",
    "\n",
    "# Print top 5 by MSE\n",
    "print_top_5(results_by_mse, \"MSE\")\n",
    "\n",
    "# Print top 5 by Absolute Correlation\n",
    "print_top_5(results_by_corr, \"Absolute Correlation\")\n",
    "\n",
    "# Best configuration by MSE\n",
    "best_config_mse, best_mse, _ = results_by_mse[0]\n",
    "print(\"\\nBest configuration by MSE:\")\n",
    "for key, value in best_config_mse.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"MSE: {best_mse:.4f}\")\n",
    "\n",
    "# Best configuration by Absolute Correlation\n",
    "best_config_corr, _, best_corr = results_by_corr[0]\n",
    "print(\"\\nBest configuration by Absolute Correlation:\")\n",
    "for key, value in best_config_corr.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Correlation: {best_corr:.4f}\")\n",
    "print(f\"|Correlation|: {abs(best_corr):.4f}\")\n",
    "\n",
    "# Diagnostic: Print predictions vs actual for best absolute correlation model\n",
    "input_size = X.shape[1]\n",
    "best_model = MLPEnsemble(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_config_corr['hidden_sizes'],\n",
    "    output_size=best_config_corr['output_size'],\n",
    "    dropout_rate=best_config_corr['dropout_rate']\n",
    ")\n",
    "best_model = train_model(best_model, torch.FloatTensor(X), torch.FloatTensor(y), torch.FloatTensor(X), torch.FloatTensor(y), \n",
    "                         epochs=best_config_corr['epochs'], patience=best_config_corr['patience'], \n",
    "                         lr=best_config_corr['lr'], weight_decay=best_config_corr['weight_decay'], seed=GLOBAL_SEED)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(torch.FloatTensor(X)).squeeze().numpy()\n",
    "    \n",
    "# print(\"\\nSample of predictions vs actual:\")\n",
    "# for i in range(min(20, len(y))):\n",
    "#     print(f\"Actual: {y[i]:.4f}, Predicted: {predictions[i]:.4f}\")\n",
    "\n",
    "# # Correlation of individual input features with target\n",
    "# print(\"\\nCorrelation of individual features with target:\")\n",
    "# for i in range(X.shape[1]):\n",
    "#     corr, _ = pearsonr(X[:, i], y)\n",
    "#     print(f\"Feature {i+1}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ef94c-7429-4239-a02c-debb3d0ab090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e0f17e4-7709-421e-9860-42cd84b943e9",
   "metadata": {},
   "source": [
    "## Amygdala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9db9039e-6c3f-4533-b00e-e615dae21044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Methods:\n",
      "Simple Average Ensemble - MSE: 0.0709, Correlation: 0.1383\n",
      "Weighted Average Ensemble - MSE: 0.1594, Correlation: 0.1758\n",
      "Non-linear Weighted Average - MSE: 0.0980, Correlation: 0.0715\n",
      "Tuned Random Forest Meta-learner - MSE: 0.0751, Correlation: 0.0333\n",
      "Stacking Ensemble - MSE: 0.0724, Correlation: 0.1399\n",
      "\n",
      "Original Models:\n",
      "DenseNet - MSE: 0.0779, Correlation: 0.1081\n",
      "PointNet - MSE: 0.0776, Correlation: 0.0420\n",
      "Random Forest - MSE: 0.0694, Correlation: 0.1986\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr, randint\n",
    "from scipy import linalg\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess_data():\n",
    "    densenet_df = pd.read_csv('densenet_amygdala_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "    pointnet_df = pd.read_csv('pointnet_amygdala_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "    random_forest_df = pd.read_csv('random_forest_pyradiomics_amygdala_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "    X = np.hstack((\n",
    "        densenet_df.iloc[:, 2:17].values,\n",
    "        pointnet_df.iloc[:, 2:17].values,\n",
    "        random_forest_df.iloc[:, 2:17].values\n",
    "    ))\n",
    "    y = densenet_df['Actual'].values\n",
    "\n",
    "    return X, y, densenet_df, pointnet_df, random_forest_df\n",
    "\n",
    "# Ensemble methods\n",
    "def simple_average_ensemble(X):\n",
    "    return np.mean(X, axis=1)\n",
    "\n",
    "def weighted_average_ensemble(X_train, y_train, X_test):\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    return lr_model.predict(X_test)\n",
    "\n",
    "def non_linear_weighted_average(X_train, y_train, X_test):\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    return mlp.predict(X_test)\n",
    "\n",
    "def tune_rf_meta_learner(X_train, y_train):\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(5, 50),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 10)\n",
    "    }\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n",
    "                                   n_iter=100, cv=5, random_state=42, n_jobs=-1)\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    return rf_random.best_estimator_\n",
    "\n",
    "def stacking_ensemble(X_train, y_train, X_test):\n",
    "    models = [\n",
    "        RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        SVR(kernel='rbf')\n",
    "    ]\n",
    "    \n",
    "    second_level_train = np.column_stack([\n",
    "        cross_val_predict(model, X_train, y_train, cv=5) \n",
    "        for model in models\n",
    "    ])\n",
    "    second_level_test = np.column_stack([\n",
    "        model.fit(X_train, y_train).predict(X_test) \n",
    "        for model in models\n",
    "    ])\n",
    "    \n",
    "    # Use Ridge regression as the meta-learner\n",
    "    alpha = 1.0\n",
    "    n_samples, n_features = second_level_train.shape\n",
    "    \n",
    "    # Add a small value to the diagonal for numerical stability\n",
    "    A = np.dot(second_level_train.T, second_level_train)\n",
    "    A.flat[::n_features + 1] += alpha + 1e-10\n",
    "    \n",
    "    b = np.dot(second_level_train.T, y_train)\n",
    "    \n",
    "    # Solve the linear system\n",
    "    try:\n",
    "        coef = linalg.solve(A, b, assume_a='pos')\n",
    "    except TypeError:\n",
    "        # For older versions of SciPy\n",
    "        coef = linalg.solve(A, b, sym_pos=True)\n",
    "    \n",
    "    # Make predictions\n",
    "    return np.dot(second_level_test, coef)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    correlation, _ = pearsonr(y_true, y_pred)\n",
    "    return mse, correlation\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    X, y, densenet_df, pointnet_df, random_forest_df = load_and_preprocess_data()\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    simple_avg_scores = []\n",
    "    weighted_avg_scores = []\n",
    "    non_linear_weighted_scores = []\n",
    "    rf_meta_scores = []\n",
    "    stacking_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Simple average\n",
    "        simple_avg_pred = simple_average_ensemble(X_test)\n",
    "        simple_avg_scores.append(evaluate_model(y_test, simple_avg_pred))\n",
    "        \n",
    "        # Weighted average\n",
    "        weighted_avg_pred = weighted_average_ensemble(X_train, y_train, X_test)\n",
    "        weighted_avg_scores.append(evaluate_model(y_test, weighted_avg_pred))\n",
    "        \n",
    "        # Non-linear weighted average\n",
    "        non_linear_weighted_pred = non_linear_weighted_average(X_train, y_train, X_test)\n",
    "        non_linear_weighted_scores.append(evaluate_model(y_test, non_linear_weighted_pred))\n",
    "        \n",
    "        # Random Forest meta-learner (with hyperparameter tuning)\n",
    "        rf_model = tune_rf_meta_learner(X_train, y_train)\n",
    "        rf_pred = rf_model.predict(X_test)\n",
    "        rf_meta_scores.append(evaluate_model(y_test, rf_pred))\n",
    "        \n",
    "        # Stacking ensemble\n",
    "        stacking_pred = stacking_ensemble(X_train, y_train, X_test)\n",
    "        stacking_scores.append(evaluate_model(y_test, stacking_pred))\n",
    "\n",
    "    # Calculate and print average scores\n",
    "    print(\"Ensemble Methods:\")\n",
    "    print(f\"Simple Average Ensemble - MSE: {np.mean([s[0] for s in simple_avg_scores]):.4f}, Correlation: {np.mean([s[1] for s in simple_avg_scores]):.4f}\")\n",
    "    print(f\"Weighted Average Ensemble - MSE: {np.mean([s[0] for s in weighted_avg_scores]):.4f}, Correlation: {np.mean([s[1] for s in weighted_avg_scores]):.4f}\")\n",
    "    print(f\"Non-linear Weighted Average - MSE: {np.mean([s[0] for s in non_linear_weighted_scores]):.4f}, Correlation: {np.mean([s[1] for s in non_linear_weighted_scores]):.4f}\")\n",
    "    print(f\"Tuned Random Forest Meta-learner - MSE: {np.mean([s[0] for s in rf_meta_scores]):.4f}, Correlation: {np.mean([s[1] for s in rf_meta_scores]):.4f}\")\n",
    "    print(f\"Stacking Ensemble - MSE: {np.mean([s[0] for s in stacking_scores]):.4f}, Correlation: {np.mean([s[1] for s in stacking_scores]):.4f}\")\n",
    "\n",
    "    # Evaluate original models\n",
    "    print(\"\\nOriginal Models:\")\n",
    "    for model_name, df in zip(['DenseNet', 'PointNet', 'Random Forest'], \n",
    "                              [densenet_df, pointnet_df, random_forest_df]):\n",
    "        avg_pred = df['Average_Prediction'].values\n",
    "        mse, correlation = evaluate_model(df['Actual'], avg_pred)\n",
    "        print(f\"{model_name} - MSE: {mse:.4f}, Correlation: {correlation:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7485bb5e-3ef6-4330-94dd-839f45a95063",
   "metadata": {},
   "source": [
    "### MLP with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4a401e6-6d42-417c-b722-b71a5b6ba24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 configurations by MSE:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0904\n",
      "   Correlation: 0.2194\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0922\n",
      "   Correlation: 0.2261\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0956\n",
      "   Correlation: 0.1369\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0969\n",
      "   Correlation: 0.1986\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0983\n",
      "   Correlation: 0.2232\n",
      "\n",
      "Top 5 configurations by Correlation:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [256]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1078\n",
      "   Correlation: 0.3040\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1423\n",
      "   Correlation: 0.2812\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1123\n",
      "   Correlation: 0.2449\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0922\n",
      "   Correlation: 0.2261\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0983\n",
      "   Correlation: 0.2232\n",
      "\n",
      "Best configuration by MSE:\n",
      "hidden_sizes: [128, 64]\n",
      "dropout_rate: 0.3\n",
      "lr: 0.0005\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "MSE: 0.0904\n",
      "\n",
      "Best configuration by Correlation:\n",
      "hidden_sizes: [256]\n",
      "dropout_rate: 0.3\n",
      "lr: 0.0005\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "Correlation: 0.3040\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a global seed\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "class MLPEnsemble(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(MLPEnsemble, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, patience, lr, weight_decay, seed):\n",
    "    set_seed(seed)  # Set seed for this training run\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).squeeze()\n",
    "    \n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "    predictions_np = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    \n",
    "    mse = mean_squared_error(y_np, predictions_np)\n",
    "    correlation, _ = pearsonr(y_np, predictions_np)\n",
    "    return mse, correlation\n",
    "\n",
    "def cross_validate_mlp(X, y, n_splits=5, **params):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    mse_scores = []\n",
    "    corr_scores = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    model_params = {\n",
    "        'input_size': X.shape[1],\n",
    "        'hidden_sizes': params['hidden_sizes'],\n",
    "        'output_size': params['output_size'],\n",
    "        'dropout_rate': params['dropout_rate']\n",
    "    }\n",
    "    train_params = {\n",
    "        'epochs': params['epochs'],\n",
    "        'patience': params['patience'],\n",
    "        'lr': params['lr'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'seed': GLOBAL_SEED\n",
    "    }\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        \n",
    "        model = MLPEnsemble(**model_params)\n",
    "        model = train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, **train_params)\n",
    "        \n",
    "        mse, corr = evaluate_model(model, X_val_tensor, y_val_tensor)\n",
    "        mse_scores.append(mse)\n",
    "        corr_scores.append(corr)\n",
    "        \n",
    "        #print(f\"Fold {fold+1} - MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    \n",
    "    return np.mean(mse_scores), np.mean(corr_scores)\n",
    "\n",
    "# Load and preprocess data\n",
    "densenet_df = pd.read_csv('densenet_amygdala_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "pointnet_df = pd.read_csv('pointnet_amygdala_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "random_forest_df = pd.read_csv('random_forest_pyradiomics_amygdala_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "X = np.hstack((\n",
    "    densenet_df.iloc[:, 2:17].values,\n",
    "    pointnet_df.iloc[:, 2:17].values,\n",
    "    random_forest_df.iloc[:, 2:17].values\n",
    "))\n",
    "y = densenet_df['Actual'].values\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32]],\n",
    "    'dropout_rate': [0, 0.3, 0.5],\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "    'epochs': [1000],\n",
    "    'patience': [50],\n",
    "    'output_size': [1]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "for params in product(*param_grid.values()):\n",
    "    config = dict(zip(param_grid.keys(), params))\n",
    "    mse, corr = cross_validate_mlp(X, y, **config)\n",
    "    results.append((config, mse, corr))\n",
    "    #print(f\"Config: {config}\")\n",
    "    #print(f\"MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "# Sort results by MSE (ascending) and positive correlation (descending)\n",
    "results_by_mse = sorted(results, key=lambda x: x[1])\n",
    "results_by_corr = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "def print_top_5(sorted_results, metric_name):\n",
    "    print(f\"\\nTop 5 configurations by {metric_name}:\")\n",
    "    for i, (config, mse, corr) in enumerate(sorted_results[:5], 1):\n",
    "        print(f\"\\n{i}. Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        print(f\"   MSE: {mse:.4f}\")\n",
    "        print(f\"   Correlation: {corr:.4f}\")\n",
    "\n",
    "# Print top 5 by MSE\n",
    "print_top_5(results_by_mse, \"MSE\")\n",
    "\n",
    "# Print top 5 by Correlation\n",
    "print_top_5(results_by_corr, \"Correlation\")\n",
    "\n",
    "# Best configuration by MSE\n",
    "best_config_mse, best_mse, _ = results_by_mse[0]\n",
    "print(\"\\nBest configuration by MSE:\")\n",
    "for key, value in best_config_mse.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"MSE: {best_mse:.4f}\")\n",
    "\n",
    "# Best configuration by Correlation\n",
    "best_config_corr, _, best_corr = results_by_corr[0]\n",
    "print(\"\\nBest configuration by Correlation:\")\n",
    "for key, value in best_config_corr.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Correlation: {best_corr:.4f}\")\n",
    "\n",
    "# Diagnostic: Print predictions vs actual for best correlation model\n",
    "input_size = X.shape[1]\n",
    "best_model = MLPEnsemble(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_config_corr['hidden_sizes'],\n",
    "    output_size=best_config_corr['output_size'],\n",
    "    dropout_rate=best_config_corr['dropout_rate']\n",
    ")\n",
    "best_model = train_model(best_model, torch.FloatTensor(X), torch.FloatTensor(y), torch.FloatTensor(X), torch.FloatTensor(y), \n",
    "                         epochs=best_config_corr['epochs'], patience=best_config_corr['patience'], \n",
    "                         lr=best_config_corr['lr'], weight_decay=best_config_corr['weight_decay'], seed=GLOBAL_SEED)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(torch.FloatTensor(X)).squeeze().numpy()\n",
    "    \n",
    "# print(\"\\nSample of predictions vs actual:\")\n",
    "# for i in range(min(20, len(y))):\n",
    "#     print(f\"Actual: {y[i]:.4f}, Predicted: {predictions[i]:.4f}\")\n",
    "\n",
    "# # Correlation of individual input features with target\n",
    "# print(\"\\nCorrelation of individual features with target:\")\n",
    "# for i in range(X.shape[1]):\n",
    "#     corr, _ = pearsonr(X[:, i], y)\n",
    "#     print(f\"Feature {i+1}: {corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3deeb16-47cb-4b96-b073-f87a6b53361b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 configurations by MSE:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0904\n",
      "   Correlation: 0.2194\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0922\n",
      "   Correlation: 0.2261\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0956\n",
      "   Correlation: 0.1369\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0969\n",
      "   Correlation: 0.1986\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0983\n",
      "   Correlation: 0.2232\n",
      "\n",
      "Top 5 configurations by Absolute Correlation:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [256]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1078\n",
      "   Correlation: 0.3040\n",
      "   |Correlation|: 0.3040\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1423\n",
      "   Correlation: 0.2812\n",
      "   |Correlation|: 0.2812\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1123\n",
      "   Correlation: 0.2449\n",
      "   |Correlation|: 0.2449\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0922\n",
      "   Correlation: 0.2261\n",
      "   |Correlation|: 0.2261\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0983\n",
      "   Correlation: 0.2232\n",
      "   |Correlation|: 0.2232\n",
      "\n",
      "Best configuration by MSE:\n",
      "hidden_sizes: [128, 64]\n",
      "dropout_rate: 0.3\n",
      "lr: 0.0005\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "MSE: 0.0904\n",
      "\n",
      "Best configuration by Absolute Correlation:\n",
      "hidden_sizes: [256]\n",
      "dropout_rate: 0.3\n",
      "lr: 0.0005\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "Correlation: 0.3040\n",
      "|Correlation|: 0.3040\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a global seed\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "class MLPEnsemble(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(MLPEnsemble, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, patience, lr, weight_decay, seed):\n",
    "    set_seed(seed)  # Set seed for this training run\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).squeeze()\n",
    "    \n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "    predictions_np = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    \n",
    "    mse = mean_squared_error(y_np, predictions_np)\n",
    "    correlation, _ = pearsonr(y_np, predictions_np)\n",
    "    return mse, correlation\n",
    "\n",
    "def cross_validate_mlp(X, y, n_splits=5, **params):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    mse_scores = []\n",
    "    corr_scores = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    model_params = {\n",
    "        'input_size': X.shape[1],\n",
    "        'hidden_sizes': params['hidden_sizes'],\n",
    "        'output_size': params['output_size'],\n",
    "        'dropout_rate': params['dropout_rate']\n",
    "    }\n",
    "    train_params = {\n",
    "        'epochs': params['epochs'],\n",
    "        'patience': params['patience'],\n",
    "        'lr': params['lr'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'seed': GLOBAL_SEED\n",
    "    }\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        \n",
    "        model = MLPEnsemble(**model_params)\n",
    "        model = train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, **train_params)\n",
    "        \n",
    "        mse, corr = evaluate_model(model, X_val_tensor, y_val_tensor)\n",
    "        mse_scores.append(mse)\n",
    "        corr_scores.append(corr)\n",
    "        \n",
    "        #print(f\"Fold {fold+1} - MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    \n",
    "    return np.mean(mse_scores), np.mean(corr_scores)\n",
    "\n",
    "# Load and preprocess data\n",
    "densenet_df = pd.read_csv('densenet_amygdala_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "pointnet_df = pd.read_csv('pointnet_amygdala_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "random_forest_df = pd.read_csv('random_forest_pyradiomics_amygdala_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "X = np.hstack((\n",
    "    densenet_df.iloc[:, 2:17].values,\n",
    "    pointnet_df.iloc[:, 2:17].values,\n",
    "    random_forest_df.iloc[:, 2:17].values\n",
    "))\n",
    "y = densenet_df['Actual'].values\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32]],\n",
    "    'dropout_rate': [0, 0.3, 0.5],\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "    'epochs': [1000],\n",
    "    'patience': [50],\n",
    "    'output_size': [1]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "for params in product(*param_grid.values()):\n",
    "    config = dict(zip(param_grid.keys(), params))\n",
    "    mse, corr = cross_validate_mlp(X, y, **config)\n",
    "    results.append((config, mse, corr))\n",
    "    #print(f\"Config: {config}\")\n",
    "    #print(f\"MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "# Sort results by MSE (ascending) and absolute correlation (descending)\n",
    "results_by_mse = sorted(results, key=lambda x: x[1])\n",
    "results_by_corr = sorted(results, key=lambda x: abs(x[2]), reverse=True)  # Modified to use absolute value\n",
    "\n",
    "def print_top_5(sorted_results, metric_name):\n",
    "    print(f\"\\nTop 5 configurations by {metric_name}:\")\n",
    "    for i, (config, mse, corr) in enumerate(sorted_results[:5], 1):\n",
    "        print(f\"\\n{i}. Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        print(f\"   MSE: {mse:.4f}\")\n",
    "        print(f\"   Correlation: {corr:.4f}\")\n",
    "        if metric_name == \"Absolute Correlation\":  # Show absolute value for clarity\n",
    "            print(f\"   |Correlation|: {abs(corr):.4f}\")\n",
    "\n",
    "# Print top 5 by MSE\n",
    "print_top_5(results_by_mse, \"MSE\")\n",
    "\n",
    "# Print top 5 by Absolute Correlation\n",
    "print_top_5(results_by_corr, \"Absolute Correlation\")\n",
    "\n",
    "# Best configuration by MSE\n",
    "best_config_mse, best_mse, _ = results_by_mse[0]\n",
    "print(\"\\nBest configuration by MSE:\")\n",
    "for key, value in best_config_mse.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"MSE: {best_mse:.4f}\")\n",
    "\n",
    "# Best configuration by Absolute Correlation\n",
    "best_config_corr, _, best_corr = results_by_corr[0]\n",
    "print(\"\\nBest configuration by Absolute Correlation:\")\n",
    "for key, value in best_config_corr.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Correlation: {best_corr:.4f}\")\n",
    "print(f\"|Correlation|: {abs(best_corr):.4f}\")\n",
    "\n",
    "# Diagnostic: Print predictions vs actual for best absolute correlation model\n",
    "input_size = X.shape[1]\n",
    "best_model = MLPEnsemble(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_config_corr['hidden_sizes'],\n",
    "    output_size=best_config_corr['output_size'],\n",
    "    dropout_rate=best_config_corr['dropout_rate']\n",
    ")\n",
    "best_model = train_model(best_model, torch.FloatTensor(X), torch.FloatTensor(y), torch.FloatTensor(X), torch.FloatTensor(y), \n",
    "                         epochs=best_config_corr['epochs'], patience=best_config_corr['patience'], \n",
    "                         lr=best_config_corr['lr'], weight_decay=best_config_corr['weight_decay'], seed=GLOBAL_SEED)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(torch.FloatTensor(X)).squeeze().numpy()\n",
    "    \n",
    "# print(\"\\nSample of predictions vs actual:\")\n",
    "# for i in range(min(20, len(y))):\n",
    "#     print(f\"Actual: {y[i]:.4f}, Predicted: {predictions[i]:.4f}\")\n",
    "\n",
    "# # Correlation of individual input features with target\n",
    "# print(\"\\nCorrelation of individual features with target:\")\n",
    "# for i in range(X.shape[1]):\n",
    "#     corr, _ = pearsonr(X[:, i], y)\n",
    "#     print(f\"Feature {i+1}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5acd1f-31bb-4cb1-a6d6-777f6afd0a4d",
   "metadata": {},
   "source": [
    "## Isthmus cingulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0327757d-c81d-4d2d-b3bf-57e950bf2ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Methods:\n",
      "Simple Average Ensemble - MSE: 0.0708, Correlation: 0.0820\n",
      "Weighted Average Ensemble - MSE: 0.2867, Correlation: -0.1368\n",
      "Non-linear Weighted Average - MSE: 0.1210, Correlation: 0.0096\n",
      "Tuned Random Forest Meta-learner - MSE: 0.0791, Correlation: -0.0689\n",
      "Stacking Ensemble - MSE: 0.0819, Correlation: -0.0554\n",
      "\n",
      "Original Models:\n",
      "DenseNet - MSE: 0.0731, Correlation: 0.1591\n",
      "PointNet - MSE: 0.0846, Correlation: -0.0114\n",
      "Random Forest - MSE: 0.0695, Correlation: 0.1990\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr, randint\n",
    "from scipy import linalg\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess_data():\n",
    "    densenet_df = pd.read_csv('densenet_isthmuscingulate_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "    pointnet_df = pd.read_csv('pointnet_isthmuscingulate_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "    random_forest_df = pd.read_csv('random_forest_pyradiomics_isthmuscingulate_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "    X = np.hstack((\n",
    "        densenet_df.iloc[:, 2:17].values,\n",
    "        pointnet_df.iloc[:, 2:17].values,\n",
    "        random_forest_df.iloc[:, 2:17].values\n",
    "    ))\n",
    "    y = densenet_df['Actual'].values\n",
    "\n",
    "    return X, y, densenet_df, pointnet_df, random_forest_df\n",
    "\n",
    "# Ensemble methods\n",
    "def simple_average_ensemble(X):\n",
    "    return np.mean(X, axis=1)\n",
    "\n",
    "def weighted_average_ensemble(X_train, y_train, X_test):\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    return lr_model.predict(X_test)\n",
    "\n",
    "def non_linear_weighted_average(X_train, y_train, X_test):\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    return mlp.predict(X_test)\n",
    "\n",
    "def tune_rf_meta_learner(X_train, y_train):\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(5, 50),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 10)\n",
    "    }\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n",
    "                                   n_iter=100, cv=5, random_state=42, n_jobs=-1)\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    return rf_random.best_estimator_\n",
    "\n",
    "def stacking_ensemble(X_train, y_train, X_test):\n",
    "    models = [\n",
    "        RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        SVR(kernel='rbf')\n",
    "    ]\n",
    "    \n",
    "    second_level_train = np.column_stack([\n",
    "        cross_val_predict(model, X_train, y_train, cv=5) \n",
    "        for model in models\n",
    "    ])\n",
    "    second_level_test = np.column_stack([\n",
    "        model.fit(X_train, y_train).predict(X_test) \n",
    "        for model in models\n",
    "    ])\n",
    "    \n",
    "    # Use Ridge regression as the meta-learner\n",
    "    alpha = 1.0\n",
    "    n_samples, n_features = second_level_train.shape\n",
    "    \n",
    "    # Add a small value to the diagonal for numerical stability\n",
    "    A = np.dot(second_level_train.T, second_level_train)\n",
    "    A.flat[::n_features + 1] += alpha + 1e-10\n",
    "    \n",
    "    b = np.dot(second_level_train.T, y_train)\n",
    "    \n",
    "    # Solve the linear system\n",
    "    try:\n",
    "        coef = linalg.solve(A, b, assume_a='pos')\n",
    "    except TypeError:\n",
    "        # For older versions of SciPy\n",
    "        coef = linalg.solve(A, b, sym_pos=True)\n",
    "    \n",
    "    # Make predictions\n",
    "    return np.dot(second_level_test, coef)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    correlation, _ = pearsonr(y_true, y_pred)\n",
    "    return mse, correlation\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    X, y, densenet_df, pointnet_df, random_forest_df = load_and_preprocess_data()\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    simple_avg_scores = []\n",
    "    weighted_avg_scores = []\n",
    "    non_linear_weighted_scores = []\n",
    "    rf_meta_scores = []\n",
    "    stacking_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Simple average\n",
    "        simple_avg_pred = simple_average_ensemble(X_test)\n",
    "        simple_avg_scores.append(evaluate_model(y_test, simple_avg_pred))\n",
    "        \n",
    "        # Weighted average\n",
    "        weighted_avg_pred = weighted_average_ensemble(X_train, y_train, X_test)\n",
    "        weighted_avg_scores.append(evaluate_model(y_test, weighted_avg_pred))\n",
    "        \n",
    "        # Non-linear weighted average\n",
    "        non_linear_weighted_pred = non_linear_weighted_average(X_train, y_train, X_test)\n",
    "        non_linear_weighted_scores.append(evaluate_model(y_test, non_linear_weighted_pred))\n",
    "        \n",
    "        # Random Forest meta-learner (with hyperparameter tuning)\n",
    "        rf_model = tune_rf_meta_learner(X_train, y_train)\n",
    "        rf_pred = rf_model.predict(X_test)\n",
    "        rf_meta_scores.append(evaluate_model(y_test, rf_pred))\n",
    "        \n",
    "        # Stacking ensemble\n",
    "        stacking_pred = stacking_ensemble(X_train, y_train, X_test)\n",
    "        stacking_scores.append(evaluate_model(y_test, stacking_pred))\n",
    "\n",
    "    # Calculate and print average scores\n",
    "    print(\"Ensemble Methods:\")\n",
    "    print(f\"Simple Average Ensemble - MSE: {np.mean([s[0] for s in simple_avg_scores]):.4f}, Correlation: {np.mean([s[1] for s in simple_avg_scores]):.4f}\")\n",
    "    print(f\"Weighted Average Ensemble - MSE: {np.mean([s[0] for s in weighted_avg_scores]):.4f}, Correlation: {np.mean([s[1] for s in weighted_avg_scores]):.4f}\")\n",
    "    print(f\"Non-linear Weighted Average - MSE: {np.mean([s[0] for s in non_linear_weighted_scores]):.4f}, Correlation: {np.mean([s[1] for s in non_linear_weighted_scores]):.4f}\")\n",
    "    print(f\"Tuned Random Forest Meta-learner - MSE: {np.mean([s[0] for s in rf_meta_scores]):.4f}, Correlation: {np.mean([s[1] for s in rf_meta_scores]):.4f}\")\n",
    "    print(f\"Stacking Ensemble - MSE: {np.mean([s[0] for s in stacking_scores]):.4f}, Correlation: {np.mean([s[1] for s in stacking_scores]):.4f}\")\n",
    "\n",
    "    # Evaluate original models\n",
    "    print(\"\\nOriginal Models:\")\n",
    "    for model_name, df in zip(['DenseNet', 'PointNet', 'Random Forest'], \n",
    "                              [densenet_df, pointnet_df, random_forest_df]):\n",
    "        avg_pred = df['Average_Prediction'].values\n",
    "        mse, correlation = evaluate_model(df['Actual'], avg_pred)\n",
    "        print(f\"{model_name} - MSE: {mse:.4f}, Correlation: {correlation:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2969aa2-e576-4174-8300-13b9d892bbfb",
   "metadata": {},
   "source": [
    "### MLP with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e87ca26c-1415-445a-bb5a-b3d949f5c2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 configurations by MSE:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0980\n",
      "   Correlation: 0.1363\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1071\n",
      "   Correlation: -0.0241\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1074\n",
      "   Correlation: 0.0023\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1091\n",
      "   Correlation: 0.1292\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1093\n",
      "   Correlation: 0.0380\n",
      "\n",
      "Top 5 configurations by Correlation:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1179\n",
      "   Correlation: 0.2449\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.3083\n",
      "   Correlation: 0.1629\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.3407\n",
      "   Correlation: 0.1517\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.5730\n",
      "   Correlation: 0.1512\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1350\n",
      "   Correlation: 0.1448\n",
      "\n",
      "Best configuration by MSE:\n",
      "hidden_sizes: [128, 64, 32]\n",
      "dropout_rate: 0.5\n",
      "lr: 0.001\n",
      "weight_decay: 0.0001\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "MSE: 0.0980\n",
      "\n",
      "Best configuration by Correlation:\n",
      "hidden_sizes: [64]\n",
      "dropout_rate: 0.3\n",
      "lr: 0.0001\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "Correlation: 0.2449\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a global seed\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "class MLPEnsemble(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(MLPEnsemble, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, patience, lr, weight_decay, seed):\n",
    "    set_seed(seed)  # Set seed for this training run\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).squeeze()\n",
    "    \n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "    predictions_np = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    \n",
    "    mse = mean_squared_error(y_np, predictions_np)\n",
    "    correlation, _ = pearsonr(y_np, predictions_np)\n",
    "    return mse, correlation\n",
    "\n",
    "def cross_validate_mlp(X, y, n_splits=5, **params):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    mse_scores = []\n",
    "    corr_scores = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    model_params = {\n",
    "        'input_size': X.shape[1],\n",
    "        'hidden_sizes': params['hidden_sizes'],\n",
    "        'output_size': params['output_size'],\n",
    "        'dropout_rate': params['dropout_rate']\n",
    "    }\n",
    "    train_params = {\n",
    "        'epochs': params['epochs'],\n",
    "        'patience': params['patience'],\n",
    "        'lr': params['lr'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'seed': GLOBAL_SEED\n",
    "    }\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        \n",
    "        model = MLPEnsemble(**model_params)\n",
    "        model = train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, **train_params)\n",
    "        \n",
    "        mse, corr = evaluate_model(model, X_val_tensor, y_val_tensor)\n",
    "        mse_scores.append(mse)\n",
    "        corr_scores.append(corr)\n",
    "        \n",
    "        #print(f\"Fold {fold+1} - MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    \n",
    "    return np.mean(mse_scores), np.mean(corr_scores)\n",
    "\n",
    "# Load and preprocess data\n",
    "densenet_df = pd.read_csv('densenet_isthmuscingulate_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "pointnet_df = pd.read_csv('pointnet_isthmuscingulate_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "random_forest_df = pd.read_csv('random_forest_pyradiomics_isthmuscingulate_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "X = np.hstack((\n",
    "    densenet_df.iloc[:, 2:17].values,\n",
    "    pointnet_df.iloc[:, 2:17].values,\n",
    "    random_forest_df.iloc[:, 2:17].values\n",
    "))\n",
    "y = densenet_df['Actual'].values\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32]],\n",
    "    'dropout_rate': [0, 0.3, 0.5],\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "    'epochs': [1000],\n",
    "    'patience': [50],\n",
    "    'output_size': [1]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "for params in product(*param_grid.values()):\n",
    "    config = dict(zip(param_grid.keys(), params))\n",
    "    mse, corr = cross_validate_mlp(X, y, **config)\n",
    "    results.append((config, mse, corr))\n",
    "    #print(f\"Config: {config}\")\n",
    "    #print(f\"MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "# Sort results by MSE (ascending) and positive correlation (descending)\n",
    "results_by_mse = sorted(results, key=lambda x: x[1])\n",
    "results_by_corr = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "def print_top_5(sorted_results, metric_name):\n",
    "    print(f\"\\nTop 5 configurations by {metric_name}:\")\n",
    "    for i, (config, mse, corr) in enumerate(sorted_results[:5], 1):\n",
    "        print(f\"\\n{i}. Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        print(f\"   MSE: {mse:.4f}\")\n",
    "        print(f\"   Correlation: {corr:.4f}\")\n",
    "\n",
    "# Print top 5 by MSE\n",
    "print_top_5(results_by_mse, \"MSE\")\n",
    "\n",
    "# Print top 5 by Correlation\n",
    "print_top_5(results_by_corr, \"Correlation\")\n",
    "\n",
    "# Best configuration by MSE\n",
    "best_config_mse, best_mse, _ = results_by_mse[0]\n",
    "print(\"\\nBest configuration by MSE:\")\n",
    "for key, value in best_config_mse.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"MSE: {best_mse:.4f}\")\n",
    "\n",
    "# Best configuration by Correlation\n",
    "best_config_corr, _, best_corr = results_by_corr[0]\n",
    "print(\"\\nBest configuration by Correlation:\")\n",
    "for key, value in best_config_corr.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Correlation: {best_corr:.4f}\")\n",
    "\n",
    "# Diagnostic: Print predictions vs actual for best correlation model\n",
    "input_size = X.shape[1]\n",
    "best_model = MLPEnsemble(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_config_corr['hidden_sizes'],\n",
    "    output_size=best_config_corr['output_size'],\n",
    "    dropout_rate=best_config_corr['dropout_rate']\n",
    ")\n",
    "best_model = train_model(best_model, torch.FloatTensor(X), torch.FloatTensor(y), torch.FloatTensor(X), torch.FloatTensor(y), \n",
    "                         epochs=best_config_corr['epochs'], patience=best_config_corr['patience'], \n",
    "                         lr=best_config_corr['lr'], weight_decay=best_config_corr['weight_decay'], seed=GLOBAL_SEED)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(torch.FloatTensor(X)).squeeze().numpy()\n",
    "    \n",
    "# print(\"\\nSample of predictions vs actual:\")\n",
    "# for i in range(min(20, len(y))):\n",
    "#     print(f\"Actual: {y[i]:.4f}, Predicted: {predictions[i]:.4f}\")\n",
    "\n",
    "# # Correlation of individual input features with target\n",
    "# print(\"\\nCorrelation of individual features with target:\")\n",
    "# for i in range(X.shape[1]):\n",
    "#     corr, _ = pearsonr(X[:, i], y)\n",
    "#     print(f\"Feature {i+1}: {corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "416087fa-e752-416b-8c77-df08e7d3eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 configurations by MSE:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0980\n",
      "   Correlation: 0.1363\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1071\n",
      "   Correlation: -0.0241\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1074\n",
      "   Correlation: 0.0023\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1091\n",
      "   Correlation: 0.1292\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1093\n",
      "   Correlation: 0.0380\n",
      "\n",
      "Top 5 configurations by Absolute Correlation:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.2117\n",
      "   Correlation: -0.3338\n",
      "   |Correlation|: 0.3338\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.2051\n",
      "   Correlation: -0.3225\n",
      "   |Correlation|: 0.3225\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0\n",
      "   lr: 0.0005\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.2463\n",
      "   Correlation: -0.3147\n",
      "   |Correlation|: 0.3147\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.2458\n",
      "   Correlation: -0.3127\n",
      "   |Correlation|: 0.3127\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [256]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1872\n",
      "   Correlation: -0.2884\n",
      "   |Correlation|: 0.2884\n",
      "\n",
      "Best configuration by MSE:\n",
      "hidden_sizes: [128, 64, 32]\n",
      "dropout_rate: 0.5\n",
      "lr: 0.001\n",
      "weight_decay: 0.0001\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "MSE: 0.0980\n",
      "\n",
      "Best configuration by Absolute Correlation:\n",
      "hidden_sizes: [64, 32]\n",
      "dropout_rate: 0\n",
      "lr: 0.001\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "Correlation: -0.3338\n",
      "|Correlation|: 0.3338\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a global seed\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "class MLPEnsemble(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(MLPEnsemble, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, patience, lr, weight_decay, seed):\n",
    "    set_seed(seed)  # Set seed for this training run\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).squeeze()\n",
    "    \n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "    predictions_np = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    \n",
    "    mse = mean_squared_error(y_np, predictions_np)\n",
    "    correlation, _ = pearsonr(y_np, predictions_np)\n",
    "    return mse, correlation\n",
    "\n",
    "def cross_validate_mlp(X, y, n_splits=5, **params):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    mse_scores = []\n",
    "    corr_scores = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    model_params = {\n",
    "        'input_size': X.shape[1],\n",
    "        'hidden_sizes': params['hidden_sizes'],\n",
    "        'output_size': params['output_size'],\n",
    "        'dropout_rate': params['dropout_rate']\n",
    "    }\n",
    "    train_params = {\n",
    "        'epochs': params['epochs'],\n",
    "        'patience': params['patience'],\n",
    "        'lr': params['lr'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'seed': GLOBAL_SEED\n",
    "    }\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        \n",
    "        model = MLPEnsemble(**model_params)\n",
    "        model = train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, **train_params)\n",
    "        \n",
    "        mse, corr = evaluate_model(model, X_val_tensor, y_val_tensor)\n",
    "        mse_scores.append(mse)\n",
    "        corr_scores.append(corr)\n",
    "        \n",
    "        #print(f\"Fold {fold+1} - MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    \n",
    "    return np.mean(mse_scores), np.mean(corr_scores)\n",
    "\n",
    "# Load and preprocess data\n",
    "densenet_df = pd.read_csv('densenet_isthmuscingulate_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "pointnet_df = pd.read_csv('pointnet_isthmuscingulate_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "random_forest_df = pd.read_csv('random_forest_pyradiomics_isthmuscingulate_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "X = np.hstack((\n",
    "    densenet_df.iloc[:, 2:17].values,\n",
    "    pointnet_df.iloc[:, 2:17].values,\n",
    "    random_forest_df.iloc[:, 2:17].values\n",
    "))\n",
    "y = densenet_df['Actual'].values\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32]],\n",
    "    'dropout_rate': [0, 0.3, 0.5],\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "    'epochs': [1000],\n",
    "    'patience': [50],\n",
    "    'output_size': [1]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "for params in product(*param_grid.values()):\n",
    "    config = dict(zip(param_grid.keys(), params))\n",
    "    mse, corr = cross_validate_mlp(X, y, **config)\n",
    "    results.append((config, mse, corr))\n",
    "    #print(f\"Config: {config}\")\n",
    "    #print(f\"MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "# Sort results by MSE (ascending) and absolute correlation (descending)\n",
    "results_by_mse = sorted(results, key=lambda x: x[1])\n",
    "results_by_corr = sorted(results, key=lambda x: abs(x[2]), reverse=True)  # Modified to use absolute value\n",
    "\n",
    "def print_top_5(sorted_results, metric_name):\n",
    "    print(f\"\\nTop 5 configurations by {metric_name}:\")\n",
    "    for i, (config, mse, corr) in enumerate(sorted_results[:5], 1):\n",
    "        print(f\"\\n{i}. Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        print(f\"   MSE: {mse:.4f}\")\n",
    "        print(f\"   Correlation: {corr:.4f}\")\n",
    "        if metric_name == \"Absolute Correlation\":  # Show absolute value for clarity\n",
    "            print(f\"   |Correlation|: {abs(corr):.4f}\")\n",
    "\n",
    "# Print top 5 by MSE\n",
    "print_top_5(results_by_mse, \"MSE\")\n",
    "\n",
    "# Print top 5 by Absolute Correlation\n",
    "print_top_5(results_by_corr, \"Absolute Correlation\")\n",
    "\n",
    "# Best configuration by MSE\n",
    "best_config_mse, best_mse, _ = results_by_mse[0]\n",
    "print(\"\\nBest configuration by MSE:\")\n",
    "for key, value in best_config_mse.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"MSE: {best_mse:.4f}\")\n",
    "\n",
    "# Best configuration by Absolute Correlation\n",
    "best_config_corr, _, best_corr = results_by_corr[0]\n",
    "print(\"\\nBest configuration by Absolute Correlation:\")\n",
    "for key, value in best_config_corr.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Correlation: {best_corr:.4f}\")\n",
    "print(f\"|Correlation|: {abs(best_corr):.4f}\")\n",
    "\n",
    "# Diagnostic: Print predictions vs actual for best absolute correlation model\n",
    "input_size = X.shape[1]\n",
    "best_model = MLPEnsemble(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_config_corr['hidden_sizes'],\n",
    "    output_size=best_config_corr['output_size'],\n",
    "    dropout_rate=best_config_corr['dropout_rate']\n",
    ")\n",
    "best_model = train_model(best_model, torch.FloatTensor(X), torch.FloatTensor(y), torch.FloatTensor(X), torch.FloatTensor(y), \n",
    "                         epochs=best_config_corr['epochs'], patience=best_config_corr['patience'], \n",
    "                         lr=best_config_corr['lr'], weight_decay=best_config_corr['weight_decay'], seed=GLOBAL_SEED)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(torch.FloatTensor(X)).squeeze().numpy()\n",
    "    \n",
    "# print(\"\\nSample of predictions vs actual:\")\n",
    "# for i in range(min(20, len(y))):\n",
    "#     print(f\"Actual: {y[i]:.4f}, Predicted: {predictions[i]:.4f}\")\n",
    "\n",
    "# # Correlation of individual input features with target\n",
    "# print(\"\\nCorrelation of individual features with target:\")\n",
    "# for i in range(X.shape[1]):\n",
    "#     corr, _ = pearsonr(X[:, i], y)\n",
    "#     print(f\"Feature {i+1}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1549fa1b-6314-470e-8273-5e7daa120583",
   "metadata": {},
   "source": [
    "## Parahippocampus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66e15f10-bd31-4dc6-8943-fa75aac0ed84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Methods:\n",
      "Simple Average Ensemble - MSE: 0.0720, Correlation: 0.0819\n",
      "Weighted Average Ensemble - MSE: 0.2188, Correlation: -0.0072\n",
      "Non-linear Weighted Average - MSE: 0.0955, Correlation: 0.2044\n",
      "Tuned Random Forest Meta-learner - MSE: 0.0754, Correlation: 0.1011\n",
      "Stacking Ensemble - MSE: 0.0739, Correlation: 0.1503\n",
      "\n",
      "Original Models:\n",
      "DenseNet - MSE: 0.0755, Correlation: 0.1722\n",
      "PointNet - MSE: 0.0862, Correlation: -0.0811\n",
      "Random Forest - MSE: 0.0688, Correlation: 0.2166\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr, randint\n",
    "from scipy import linalg\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess_data():\n",
    "    densenet_df = pd.read_csv('densenet_parahippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "    pointnet_df = pd.read_csv('pointnet_parahippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "    random_forest_df = pd.read_csv('random_forest_pyradiomics_parahippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "    X = np.hstack((\n",
    "        densenet_df.iloc[:, 2:17].values,\n",
    "        pointnet_df.iloc[:, 2:17].values,\n",
    "        random_forest_df.iloc[:, 2:17].values\n",
    "    ))\n",
    "    y = densenet_df['Actual'].values\n",
    "\n",
    "    return X, y, densenet_df, pointnet_df, random_forest_df\n",
    "\n",
    "# Ensemble methods\n",
    "def simple_average_ensemble(X):\n",
    "    return np.mean(X, axis=1)\n",
    "\n",
    "def weighted_average_ensemble(X_train, y_train, X_test):\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    return lr_model.predict(X_test)\n",
    "\n",
    "def non_linear_weighted_average(X_train, y_train, X_test):\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    return mlp.predict(X_test)\n",
    "\n",
    "def tune_rf_meta_learner(X_train, y_train):\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(5, 50),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 10)\n",
    "    }\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n",
    "                                   n_iter=100, cv=5, random_state=42, n_jobs=-1)\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    return rf_random.best_estimator_\n",
    "\n",
    "def stacking_ensemble(X_train, y_train, X_test):\n",
    "    models = [\n",
    "        RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        SVR(kernel='rbf')\n",
    "    ]\n",
    "    \n",
    "    second_level_train = np.column_stack([\n",
    "        cross_val_predict(model, X_train, y_train, cv=5) \n",
    "        for model in models\n",
    "    ])\n",
    "    second_level_test = np.column_stack([\n",
    "        model.fit(X_train, y_train).predict(X_test) \n",
    "        for model in models\n",
    "    ])\n",
    "    \n",
    "    # Use Ridge regression as the meta-learner\n",
    "    alpha = 1.0\n",
    "    n_samples, n_features = second_level_train.shape\n",
    "    \n",
    "    # Add a small value to the diagonal for numerical stability\n",
    "    A = np.dot(second_level_train.T, second_level_train)\n",
    "    A.flat[::n_features + 1] += alpha + 1e-10\n",
    "    \n",
    "    b = np.dot(second_level_train.T, y_train)\n",
    "    \n",
    "    # Solve the linear system\n",
    "    try:\n",
    "        coef = linalg.solve(A, b, assume_a='pos')\n",
    "    except TypeError:\n",
    "        # For older versions of SciPy\n",
    "        coef = linalg.solve(A, b, sym_pos=True)\n",
    "    \n",
    "    # Make predictions\n",
    "    return np.dot(second_level_test, coef)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    correlation, _ = pearsonr(y_true, y_pred)\n",
    "    return mse, correlation\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    X, y, densenet_df, pointnet_df, random_forest_df = load_and_preprocess_data()\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    simple_avg_scores = []\n",
    "    weighted_avg_scores = []\n",
    "    non_linear_weighted_scores = []\n",
    "    rf_meta_scores = []\n",
    "    stacking_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Simple average\n",
    "        simple_avg_pred = simple_average_ensemble(X_test)\n",
    "        simple_avg_scores.append(evaluate_model(y_test, simple_avg_pred))\n",
    "        \n",
    "        # Weighted average\n",
    "        weighted_avg_pred = weighted_average_ensemble(X_train, y_train, X_test)\n",
    "        weighted_avg_scores.append(evaluate_model(y_test, weighted_avg_pred))\n",
    "        \n",
    "        # Non-linear weighted average\n",
    "        non_linear_weighted_pred = non_linear_weighted_average(X_train, y_train, X_test)\n",
    "        non_linear_weighted_scores.append(evaluate_model(y_test, non_linear_weighted_pred))\n",
    "        \n",
    "        # Random Forest meta-learner (with hyperparameter tuning)\n",
    "        rf_model = tune_rf_meta_learner(X_train, y_train)\n",
    "        rf_pred = rf_model.predict(X_test)\n",
    "        rf_meta_scores.append(evaluate_model(y_test, rf_pred))\n",
    "        \n",
    "        # Stacking ensemble\n",
    "        stacking_pred = stacking_ensemble(X_train, y_train, X_test)\n",
    "        stacking_scores.append(evaluate_model(y_test, stacking_pred))\n",
    "\n",
    "    # Calculate and print average scores\n",
    "    print(\"Ensemble Methods:\")\n",
    "    print(f\"Simple Average Ensemble - MSE: {np.mean([s[0] for s in simple_avg_scores]):.4f}, Correlation: {np.mean([s[1] for s in simple_avg_scores]):.4f}\")\n",
    "    print(f\"Weighted Average Ensemble - MSE: {np.mean([s[0] for s in weighted_avg_scores]):.4f}, Correlation: {np.mean([s[1] for s in weighted_avg_scores]):.4f}\")\n",
    "    print(f\"Non-linear Weighted Average - MSE: {np.mean([s[0] for s in non_linear_weighted_scores]):.4f}, Correlation: {np.mean([s[1] for s in non_linear_weighted_scores]):.4f}\")\n",
    "    print(f\"Tuned Random Forest Meta-learner - MSE: {np.mean([s[0] for s in rf_meta_scores]):.4f}, Correlation: {np.mean([s[1] for s in rf_meta_scores]):.4f}\")\n",
    "    print(f\"Stacking Ensemble - MSE: {np.mean([s[0] for s in stacking_scores]):.4f}, Correlation: {np.mean([s[1] for s in stacking_scores]):.4f}\")\n",
    "\n",
    "    # Evaluate original models\n",
    "    print(\"\\nOriginal Models:\")\n",
    "    for model_name, df in zip(['DenseNet', 'PointNet', 'Random Forest'], \n",
    "                              [densenet_df, pointnet_df, random_forest_df]):\n",
    "        avg_pred = df['Average_Prediction'].values\n",
    "        mse, correlation = evaluate_model(df['Actual'], avg_pred)\n",
    "        print(f\"{model_name} - MSE: {mse:.4f}, Correlation: {correlation:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f9ce98-6b75-4078-b739-59f76e3fbf6c",
   "metadata": {},
   "source": [
    "### MLP with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d3b08f8-76c1-4ddf-99fa-35ac4f28210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 configurations by MSE:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0846\n",
      "   Correlation: 0.3092\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0917\n",
      "   Correlation: 0.1818\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0932\n",
      "   Correlation: 0.3336\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0952\n",
      "   Correlation: 0.1974\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1015\n",
      "   Correlation: 0.1923\n",
      "\n",
      "Top 5 configurations by Correlation:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0932\n",
      "   Correlation: 0.3336\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0846\n",
      "   Correlation: 0.3092\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1036\n",
      "   Correlation: 0.3045\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.2496\n",
      "   Correlation: 0.2558\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1954\n",
      "   Correlation: 0.1980\n",
      "\n",
      "Best configuration by MSE:\n",
      "hidden_sizes: [256, 128]\n",
      "dropout_rate: 0.3\n",
      "lr: 0.001\n",
      "weight_decay: 0.0001\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "MSE: 0.0846\n",
      "\n",
      "Best configuration by Correlation:\n",
      "hidden_sizes: [256, 128]\n",
      "dropout_rate: 0.3\n",
      "lr: 0.001\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "Correlation: 0.3336\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a global seed\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "class MLPEnsemble(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(MLPEnsemble, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, patience, lr, weight_decay, seed):\n",
    "    set_seed(seed)  # Set seed for this training run\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).squeeze()\n",
    "    \n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "    predictions_np = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    \n",
    "    mse = mean_squared_error(y_np, predictions_np)\n",
    "    correlation, _ = pearsonr(y_np, predictions_np)\n",
    "    return mse, correlation\n",
    "\n",
    "def cross_validate_mlp(X, y, n_splits=5, **params):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    mse_scores = []\n",
    "    corr_scores = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    model_params = {\n",
    "        'input_size': X.shape[1],\n",
    "        'hidden_sizes': params['hidden_sizes'],\n",
    "        'output_size': params['output_size'],\n",
    "        'dropout_rate': params['dropout_rate']\n",
    "    }\n",
    "    train_params = {\n",
    "        'epochs': params['epochs'],\n",
    "        'patience': params['patience'],\n",
    "        'lr': params['lr'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'seed': GLOBAL_SEED\n",
    "    }\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        \n",
    "        model = MLPEnsemble(**model_params)\n",
    "        model = train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, **train_params)\n",
    "        \n",
    "        mse, corr = evaluate_model(model, X_val_tensor, y_val_tensor)\n",
    "        mse_scores.append(mse)\n",
    "        corr_scores.append(corr)\n",
    "        \n",
    "        #print(f\"Fold {fold+1} - MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    \n",
    "    return np.mean(mse_scores), np.mean(corr_scores)\n",
    "\n",
    "# Load and preprocess data\n",
    "densenet_df = pd.read_csv('densenet_parahippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "pointnet_df = pd.read_csv('pointnet_parahippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "random_forest_df = pd.read_csv('random_forest_pyradiomics_parahippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "X = np.hstack((\n",
    "    densenet_df.iloc[:, 2:17].values,\n",
    "    pointnet_df.iloc[:, 2:17].values,\n",
    "    random_forest_df.iloc[:, 2:17].values\n",
    "))\n",
    "y = densenet_df['Actual'].values\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32]],\n",
    "    'dropout_rate': [0, 0.3, 0.5],\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "    'epochs': [1000],\n",
    "    'patience': [50],\n",
    "    'output_size': [1]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "for params in product(*param_grid.values()):\n",
    "    config = dict(zip(param_grid.keys(), params))\n",
    "    mse, corr = cross_validate_mlp(X, y, **config)\n",
    "    results.append((config, mse, corr))\n",
    "    #print(f\"Config: {config}\")\n",
    "    #print(f\"MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "# Sort results by MSE (ascending) and positive correlation (descending)\n",
    "results_by_mse = sorted(results, key=lambda x: x[1])\n",
    "results_by_corr = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "def print_top_5(sorted_results, metric_name):\n",
    "    print(f\"\\nTop 5 configurations by {metric_name}:\")\n",
    "    for i, (config, mse, corr) in enumerate(sorted_results[:5], 1):\n",
    "        print(f\"\\n{i}. Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        print(f\"   MSE: {mse:.4f}\")\n",
    "        print(f\"   Correlation: {corr:.4f}\")\n",
    "\n",
    "# Print top 5 by MSE\n",
    "print_top_5(results_by_mse, \"MSE\")\n",
    "\n",
    "# Print top 5 by Correlation\n",
    "print_top_5(results_by_corr, \"Correlation\")\n",
    "\n",
    "# Best configuration by MSE\n",
    "best_config_mse, best_mse, _ = results_by_mse[0]\n",
    "print(\"\\nBest configuration by MSE:\")\n",
    "for key, value in best_config_mse.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"MSE: {best_mse:.4f}\")\n",
    "\n",
    "# Best configuration by Correlation\n",
    "best_config_corr, _, best_corr = results_by_corr[0]\n",
    "print(\"\\nBest configuration by Correlation:\")\n",
    "for key, value in best_config_corr.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Correlation: {best_corr:.4f}\")\n",
    "\n",
    "# Diagnostic: Print predictions vs actual for best correlation model\n",
    "input_size = X.shape[1]\n",
    "best_model = MLPEnsemble(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_config_corr['hidden_sizes'],\n",
    "    output_size=best_config_corr['output_size'],\n",
    "    dropout_rate=best_config_corr['dropout_rate']\n",
    ")\n",
    "best_model = train_model(best_model, torch.FloatTensor(X), torch.FloatTensor(y), torch.FloatTensor(X), torch.FloatTensor(y), \n",
    "                         epochs=best_config_corr['epochs'], patience=best_config_corr['patience'], \n",
    "                         lr=best_config_corr['lr'], weight_decay=best_config_corr['weight_decay'], seed=GLOBAL_SEED)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(torch.FloatTensor(X)).squeeze().numpy()\n",
    "    \n",
    "# print(\"\\nSample of predictions vs actual:\")\n",
    "# for i in range(min(20, len(y))):\n",
    "#     print(f\"Actual: {y[i]:.4f}, Predicted: {predictions[i]:.4f}\")\n",
    "\n",
    "# # Correlation of individual input features with target\n",
    "# print(\"\\nCorrelation of individual features with target:\")\n",
    "# for i in range(X.shape[1]):\n",
    "#     corr, _ = pearsonr(X[:, i], y)\n",
    "#     print(f\"Feature {i+1}: {corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aebcd805-1f90-42a7-ae10-617367ddb637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 configurations by MSE:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0846\n",
      "   Correlation: 0.3092\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0917\n",
      "   Correlation: 0.1818\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0932\n",
      "   Correlation: 0.3336\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0952\n",
      "   Correlation: 0.1974\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1015\n",
      "   Correlation: 0.1923\n",
      "\n",
      "Top 5 configurations by Absolute Correlation:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0932\n",
      "   Correlation: 0.3336\n",
      "   |Correlation|: 0.3336\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0846\n",
      "   Correlation: 0.3092\n",
      "   |Correlation|: 0.3092\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1036\n",
      "   Correlation: 0.3045\n",
      "   |Correlation|: 0.3045\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.2496\n",
      "   Correlation: 0.2558\n",
      "   |Correlation|: 0.2558\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [256]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1596\n",
      "   Correlation: -0.2054\n",
      "   |Correlation|: 0.2054\n",
      "\n",
      "Best configuration by MSE:\n",
      "hidden_sizes: [256, 128]\n",
      "dropout_rate: 0.3\n",
      "lr: 0.001\n",
      "weight_decay: 0.0001\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "MSE: 0.0846\n",
      "\n",
      "Best configuration by Absolute Correlation:\n",
      "hidden_sizes: [256, 128]\n",
      "dropout_rate: 0.3\n",
      "lr: 0.001\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "Correlation: 0.3336\n",
      "|Correlation|: 0.3336\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a global seed\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "class MLPEnsemble(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(MLPEnsemble, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, patience, lr, weight_decay, seed):\n",
    "    set_seed(seed)  # Set seed for this training run\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).squeeze()\n",
    "    \n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "    predictions_np = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    \n",
    "    mse = mean_squared_error(y_np, predictions_np)\n",
    "    correlation, _ = pearsonr(y_np, predictions_np)\n",
    "    return mse, correlation\n",
    "\n",
    "def cross_validate_mlp(X, y, n_splits=5, **params):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    mse_scores = []\n",
    "    corr_scores = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    model_params = {\n",
    "        'input_size': X.shape[1],\n",
    "        'hidden_sizes': params['hidden_sizes'],\n",
    "        'output_size': params['output_size'],\n",
    "        'dropout_rate': params['dropout_rate']\n",
    "    }\n",
    "    train_params = {\n",
    "        'epochs': params['epochs'],\n",
    "        'patience': params['patience'],\n",
    "        'lr': params['lr'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'seed': GLOBAL_SEED\n",
    "    }\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        \n",
    "        model = MLPEnsemble(**model_params)\n",
    "        model = train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, **train_params)\n",
    "        \n",
    "        mse, corr = evaluate_model(model, X_val_tensor, y_val_tensor)\n",
    "        mse_scores.append(mse)\n",
    "        corr_scores.append(corr)\n",
    "        \n",
    "        #print(f\"Fold {fold+1} - MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    \n",
    "    return np.mean(mse_scores), np.mean(corr_scores)\n",
    "\n",
    "# Load and preprocess data\n",
    "densenet_df = pd.read_csv('densenet_parahippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "pointnet_df = pd.read_csv('pointnet_parahippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "random_forest_df = pd.read_csv('random_forest_pyradiomics_parahippocampus_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "X = np.hstack((\n",
    "    densenet_df.iloc[:, 2:17].values,\n",
    "    pointnet_df.iloc[:, 2:17].values,\n",
    "    random_forest_df.iloc[:, 2:17].values\n",
    "))\n",
    "y = densenet_df['Actual'].values\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32]],\n",
    "    'dropout_rate': [0, 0.3, 0.5],\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "    'epochs': [1000],\n",
    "    'patience': [50],\n",
    "    'output_size': [1]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "for params in product(*param_grid.values()):\n",
    "    config = dict(zip(param_grid.keys(), params))\n",
    "    mse, corr = cross_validate_mlp(X, y, **config)\n",
    "    results.append((config, mse, corr))\n",
    "    #print(f\"Config: {config}\")\n",
    "    #print(f\"MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "# Sort results by MSE (ascending) and absolute correlation (descending)\n",
    "results_by_mse = sorted(results, key=lambda x: x[1])\n",
    "results_by_corr = sorted(results, key=lambda x: abs(x[2]), reverse=True)  # Modified to use absolute value\n",
    "\n",
    "def print_top_5(sorted_results, metric_name):\n",
    "    print(f\"\\nTop 5 configurations by {metric_name}:\")\n",
    "    for i, (config, mse, corr) in enumerate(sorted_results[:5], 1):\n",
    "        print(f\"\\n{i}. Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        print(f\"   MSE: {mse:.4f}\")\n",
    "        print(f\"   Correlation: {corr:.4f}\")\n",
    "        if metric_name == \"Absolute Correlation\":  # Show absolute value for clarity\n",
    "            print(f\"   |Correlation|: {abs(corr):.4f}\")\n",
    "\n",
    "# Print top 5 by MSE\n",
    "print_top_5(results_by_mse, \"MSE\")\n",
    "\n",
    "# Print top 5 by Absolute Correlation\n",
    "print_top_5(results_by_corr, \"Absolute Correlation\")\n",
    "\n",
    "# Best configuration by MSE\n",
    "best_config_mse, best_mse, _ = results_by_mse[0]\n",
    "print(\"\\nBest configuration by MSE:\")\n",
    "for key, value in best_config_mse.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"MSE: {best_mse:.4f}\")\n",
    "\n",
    "# Best configuration by Absolute Correlation\n",
    "best_config_corr, _, best_corr = results_by_corr[0]\n",
    "print(\"\\nBest configuration by Absolute Correlation:\")\n",
    "for key, value in best_config_corr.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Correlation: {best_corr:.4f}\")\n",
    "print(f\"|Correlation|: {abs(best_corr):.4f}\")\n",
    "\n",
    "# Diagnostic: Print predictions vs actual for best absolute correlation model\n",
    "input_size = X.shape[1]\n",
    "best_model = MLPEnsemble(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_config_corr['hidden_sizes'],\n",
    "    output_size=best_config_corr['output_size'],\n",
    "    dropout_rate=best_config_corr['dropout_rate']\n",
    ")\n",
    "best_model = train_model(best_model, torch.FloatTensor(X), torch.FloatTensor(y), torch.FloatTensor(X), torch.FloatTensor(y), \n",
    "                         epochs=best_config_corr['epochs'], patience=best_config_corr['patience'], \n",
    "                         lr=best_config_corr['lr'], weight_decay=best_config_corr['weight_decay'], seed=GLOBAL_SEED)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(torch.FloatTensor(X)).squeeze().numpy()\n",
    "    \n",
    "# print(\"\\nSample of predictions vs actual:\")\n",
    "# for i in range(min(20, len(y))):\n",
    "#     print(f\"Actual: {y[i]:.4f}, Predicted: {predictions[i]:.4f}\")\n",
    "\n",
    "# # Correlation of individual input features with target\n",
    "# print(\"\\nCorrelation of individual features with target:\")\n",
    "# for i in range(X.shape[1]):\n",
    "#     corr, _ = pearsonr(X[:, i], y)\n",
    "#     print(f\"Feature {i+1}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b419d-61b9-46ac-a52f-7d01f8cb360a",
   "metadata": {},
   "source": [
    "## Entorhinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3206b12-f9f4-4282-a5ca-d0effab49f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Methods:\n",
      "Simple Average Ensemble - MSE: 0.0761, Correlation: -0.0941\n",
      "Weighted Average Ensemble - MSE: 0.2303, Correlation: -0.0990\n",
      "Non-linear Weighted Average - MSE: 0.1107, Correlation: 0.0444\n",
      "Tuned Random Forest Meta-learner - MSE: 0.0711, Correlation: 0.1850\n",
      "Stacking Ensemble - MSE: 0.0738, Correlation: 0.1365\n",
      "\n",
      "Original Models:\n",
      "DenseNet - MSE: 0.0841, Correlation: -0.0657\n",
      "PointNet - MSE: 0.0828, Correlation: -0.1635\n",
      "Random Forest - MSE: 0.0714, Correlation: 0.1222\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr, randint\n",
    "from scipy import linalg\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess_data():\n",
    "    densenet_df = pd.read_csv('densenet_entorhinal_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "    pointnet_df = pd.read_csv('pointnet_entorhinal_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "    random_forest_df = pd.read_csv('random_forest_pyradiomics_entorhinal_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "    X = np.hstack((\n",
    "        densenet_df.iloc[:, 2:17].values,\n",
    "        pointnet_df.iloc[:, 2:17].values,\n",
    "        random_forest_df.iloc[:, 2:17].values\n",
    "    ))\n",
    "    y = densenet_df['Actual'].values\n",
    "\n",
    "    return X, y, densenet_df, pointnet_df, random_forest_df\n",
    "\n",
    "# Ensemble methods\n",
    "def simple_average_ensemble(X):\n",
    "    return np.mean(X, axis=1)\n",
    "\n",
    "def weighted_average_ensemble(X_train, y_train, X_test):\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    return lr_model.predict(X_test)\n",
    "\n",
    "def non_linear_weighted_average(X_train, y_train, X_test):\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    return mlp.predict(X_test)\n",
    "\n",
    "def tune_rf_meta_learner(X_train, y_train):\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(5, 50),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 10)\n",
    "    }\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n",
    "                                   n_iter=100, cv=5, random_state=42, n_jobs=-1)\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    return rf_random.best_estimator_\n",
    "\n",
    "def stacking_ensemble(X_train, y_train, X_test):\n",
    "    models = [\n",
    "        RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        SVR(kernel='rbf')\n",
    "    ]\n",
    "    \n",
    "    second_level_train = np.column_stack([\n",
    "        cross_val_predict(model, X_train, y_train, cv=5) \n",
    "        for model in models\n",
    "    ])\n",
    "    second_level_test = np.column_stack([\n",
    "        model.fit(X_train, y_train).predict(X_test) \n",
    "        for model in models\n",
    "    ])\n",
    "    \n",
    "    # Use Ridge regression as the meta-learner\n",
    "    alpha = 1.0\n",
    "    n_samples, n_features = second_level_train.shape\n",
    "    \n",
    "    # Add a small value to the diagonal for numerical stability\n",
    "    A = np.dot(second_level_train.T, second_level_train)\n",
    "    A.flat[::n_features + 1] += alpha + 1e-10\n",
    "    \n",
    "    b = np.dot(second_level_train.T, y_train)\n",
    "    \n",
    "    # Solve the linear system\n",
    "    try:\n",
    "        coef = linalg.solve(A, b, assume_a='pos')\n",
    "    except TypeError:\n",
    "        # For older versions of SciPy\n",
    "        coef = linalg.solve(A, b, sym_pos=True)\n",
    "    \n",
    "    # Make predictions\n",
    "    return np.dot(second_level_test, coef)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    correlation, _ = pearsonr(y_true, y_pred)\n",
    "    return mse, correlation\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    X, y, densenet_df, pointnet_df, random_forest_df = load_and_preprocess_data()\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    simple_avg_scores = []\n",
    "    weighted_avg_scores = []\n",
    "    non_linear_weighted_scores = []\n",
    "    rf_meta_scores = []\n",
    "    stacking_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Simple average\n",
    "        simple_avg_pred = simple_average_ensemble(X_test)\n",
    "        simple_avg_scores.append(evaluate_model(y_test, simple_avg_pred))\n",
    "        \n",
    "        # Weighted average\n",
    "        weighted_avg_pred = weighted_average_ensemble(X_train, y_train, X_test)\n",
    "        weighted_avg_scores.append(evaluate_model(y_test, weighted_avg_pred))\n",
    "        \n",
    "        # Non-linear weighted average\n",
    "        non_linear_weighted_pred = non_linear_weighted_average(X_train, y_train, X_test)\n",
    "        non_linear_weighted_scores.append(evaluate_model(y_test, non_linear_weighted_pred))\n",
    "        \n",
    "        # Random Forest meta-learner (with hyperparameter tuning)\n",
    "        rf_model = tune_rf_meta_learner(X_train, y_train)\n",
    "        rf_pred = rf_model.predict(X_test)\n",
    "        rf_meta_scores.append(evaluate_model(y_test, rf_pred))\n",
    "        \n",
    "        # Stacking ensemble\n",
    "        stacking_pred = stacking_ensemble(X_train, y_train, X_test)\n",
    "        stacking_scores.append(evaluate_model(y_test, stacking_pred))\n",
    "\n",
    "    # Calculate and print average scores\n",
    "    print(\"Ensemble Methods:\")\n",
    "    print(f\"Simple Average Ensemble - MSE: {np.mean([s[0] for s in simple_avg_scores]):.4f}, Correlation: {np.mean([s[1] for s in simple_avg_scores]):.4f}\")\n",
    "    print(f\"Weighted Average Ensemble - MSE: {np.mean([s[0] for s in weighted_avg_scores]):.4f}, Correlation: {np.mean([s[1] for s in weighted_avg_scores]):.4f}\")\n",
    "    print(f\"Non-linear Weighted Average - MSE: {np.mean([s[0] for s in non_linear_weighted_scores]):.4f}, Correlation: {np.mean([s[1] for s in non_linear_weighted_scores]):.4f}\")\n",
    "    print(f\"Tuned Random Forest Meta-learner - MSE: {np.mean([s[0] for s in rf_meta_scores]):.4f}, Correlation: {np.mean([s[1] for s in rf_meta_scores]):.4f}\")\n",
    "    print(f\"Stacking Ensemble - MSE: {np.mean([s[0] for s in stacking_scores]):.4f}, Correlation: {np.mean([s[1] for s in stacking_scores]):.4f}\")\n",
    "\n",
    "    # Evaluate original models\n",
    "    print(\"\\nOriginal Models:\")\n",
    "    for model_name, df in zip(['DenseNet', 'PointNet', 'Random Forest'], \n",
    "                              [densenet_df, pointnet_df, random_forest_df]):\n",
    "        avg_pred = df['Average_Prediction'].values\n",
    "        mse, correlation = evaluate_model(df['Actual'], avg_pred)\n",
    "        print(f\"{model_name} - MSE: {mse:.4f}, Correlation: {correlation:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575c2ce-bc94-4175-acb2-63a4e35c8e8a",
   "metadata": {},
   "source": [
    "### MLP with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d12a195-52c7-44ce-b470-9276a1da980a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 configurations by MSE:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0926\n",
      "   Correlation: 0.1728\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0948\n",
      "   Correlation: 0.0486\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0973\n",
      "   Correlation: 0.1017\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1015\n",
      "   Correlation: 0.1772\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1031\n",
      "   Correlation: 0.0477\n",
      "\n",
      "Top 5 configurations by Correlation:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.2052\n",
      "   Correlation: 0.2082\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1168\n",
      "   Correlation: 0.2034\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1102\n",
      "   Correlation: 0.2025\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1015\n",
      "   Correlation: 0.1772\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0926\n",
      "   Correlation: 0.1728\n",
      "\n",
      "Best configuration by MSE:\n",
      "hidden_sizes: [64]\n",
      "dropout_rate: 0.5\n",
      "lr: 0.001\n",
      "weight_decay: 0.0001\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "MSE: 0.0926\n",
      "\n",
      "Best configuration by Correlation:\n",
      "hidden_sizes: [256, 128]\n",
      "dropout_rate: 0.3\n",
      "lr: 0.0001\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "Correlation: 0.2082\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a global seed\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "class MLPEnsemble(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(MLPEnsemble, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, patience, lr, weight_decay, seed):\n",
    "    set_seed(seed)  # Set seed for this training run\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).squeeze()\n",
    "    \n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "    predictions_np = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    \n",
    "    mse = mean_squared_error(y_np, predictions_np)\n",
    "    correlation, _ = pearsonr(y_np, predictions_np)\n",
    "    return mse, correlation\n",
    "\n",
    "def cross_validate_mlp(X, y, n_splits=5, **params):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    mse_scores = []\n",
    "    corr_scores = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    model_params = {\n",
    "        'input_size': X.shape[1],\n",
    "        'hidden_sizes': params['hidden_sizes'],\n",
    "        'output_size': params['output_size'],\n",
    "        'dropout_rate': params['dropout_rate']\n",
    "    }\n",
    "    train_params = {\n",
    "        'epochs': params['epochs'],\n",
    "        'patience': params['patience'],\n",
    "        'lr': params['lr'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'seed': GLOBAL_SEED\n",
    "    }\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        \n",
    "        model = MLPEnsemble(**model_params)\n",
    "        model = train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, **train_params)\n",
    "        \n",
    "        mse, corr = evaluate_model(model, X_val_tensor, y_val_tensor)\n",
    "        mse_scores.append(mse)\n",
    "        corr_scores.append(corr)\n",
    "        \n",
    "        #print(f\"Fold {fold+1} - MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    \n",
    "    return np.mean(mse_scores), np.mean(corr_scores)\n",
    "\n",
    "# Load and preprocess data\n",
    "densenet_df = pd.read_csv('densenet_entorhinal_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "pointnet_df = pd.read_csv('pointnet_entorhinal_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "random_forest_df = pd.read_csv('random_forest_pyradiomics_entorhinal_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "X = np.hstack((\n",
    "    densenet_df.iloc[:, 2:17].values,\n",
    "    pointnet_df.iloc[:, 2:17].values,\n",
    "    random_forest_df.iloc[:, 2:17].values\n",
    "))\n",
    "y = densenet_df['Actual'].values\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32]],\n",
    "    'dropout_rate': [0, 0.3, 0.5],\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "    'epochs': [1000],\n",
    "    'patience': [50],\n",
    "    'output_size': [1]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "for params in product(*param_grid.values()):\n",
    "    config = dict(zip(param_grid.keys(), params))\n",
    "    mse, corr = cross_validate_mlp(X, y, **config)\n",
    "    results.append((config, mse, corr))\n",
    "    #print(f\"Config: {config}\")\n",
    "    #print(f\"MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "# Sort results by MSE (ascending) and positive correlation (descending)\n",
    "results_by_mse = sorted(results, key=lambda x: x[1])\n",
    "results_by_corr = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "def print_top_5(sorted_results, metric_name):\n",
    "    print(f\"\\nTop 5 configurations by {metric_name}:\")\n",
    "    for i, (config, mse, corr) in enumerate(sorted_results[:5], 1):\n",
    "        print(f\"\\n{i}. Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        print(f\"   MSE: {mse:.4f}\")\n",
    "        print(f\"   Correlation: {corr:.4f}\")\n",
    "\n",
    "# Print top 5 by MSE\n",
    "print_top_5(results_by_mse, \"MSE\")\n",
    "\n",
    "# Print top 5 by Correlation\n",
    "print_top_5(results_by_corr, \"Correlation\")\n",
    "\n",
    "# Best configuration by MSE\n",
    "best_config_mse, best_mse, _ = results_by_mse[0]\n",
    "print(\"\\nBest configuration by MSE:\")\n",
    "for key, value in best_config_mse.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"MSE: {best_mse:.4f}\")\n",
    "\n",
    "# Best configuration by Correlation\n",
    "best_config_corr, _, best_corr = results_by_corr[0]\n",
    "print(\"\\nBest configuration by Correlation:\")\n",
    "for key, value in best_config_corr.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Correlation: {best_corr:.4f}\")\n",
    "\n",
    "# Diagnostic: Print predictions vs actual for best correlation model\n",
    "input_size = X.shape[1]\n",
    "best_model = MLPEnsemble(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_config_corr['hidden_sizes'],\n",
    "    output_size=best_config_corr['output_size'],\n",
    "    dropout_rate=best_config_corr['dropout_rate']\n",
    ")\n",
    "best_model = train_model(best_model, torch.FloatTensor(X), torch.FloatTensor(y), torch.FloatTensor(X), torch.FloatTensor(y), \n",
    "                         epochs=best_config_corr['epochs'], patience=best_config_corr['patience'], \n",
    "                         lr=best_config_corr['lr'], weight_decay=best_config_corr['weight_decay'], seed=GLOBAL_SEED)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(torch.FloatTensor(X)).squeeze().numpy()\n",
    "    \n",
    "# print(\"\\nSample of predictions vs actual:\")\n",
    "# for i in range(min(20, len(y))):\n",
    "#     print(f\"Actual: {y[i]:.4f}, Predicted: {predictions[i]:.4f}\")\n",
    "\n",
    "# # Correlation of individual input features with target\n",
    "# print(\"\\nCorrelation of individual features with target:\")\n",
    "# for i in range(X.shape[1]):\n",
    "#     corr, _ = pearsonr(X[:, i], y)\n",
    "#     print(f\"Feature {i+1}: {corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af2c372f-9073-4407-81d8-7f63beb926a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 configurations by MSE:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0926\n",
      "   Correlation: 0.1728\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0948\n",
      "   Correlation: 0.0486\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0973\n",
      "   Correlation: 0.1017\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1015\n",
      "   Correlation: 0.1772\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1031\n",
      "   Correlation: 0.0477\n",
      "\n",
      "Top 5 configurations by Absolute Correlation:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1561\n",
      "   Correlation: -0.3136\n",
      "   |Correlation|: 0.3136\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.2052\n",
      "   Correlation: 0.2082\n",
      "   |Correlation|: 0.2082\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1168\n",
      "   Correlation: 0.2034\n",
      "   |Correlation|: 0.2034\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1102\n",
      "   Correlation: 0.2025\n",
      "   |Correlation|: 0.2025\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [256, 128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1015\n",
      "   Correlation: 0.1772\n",
      "   |Correlation|: 0.1772\n",
      "\n",
      "Best configuration by MSE:\n",
      "hidden_sizes: [64]\n",
      "dropout_rate: 0.5\n",
      "lr: 0.001\n",
      "weight_decay: 0.0001\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "MSE: 0.0926\n",
      "\n",
      "Best configuration by Absolute Correlation:\n",
      "hidden_sizes: [64, 32]\n",
      "dropout_rate: 0.3\n",
      "lr: 0.0005\n",
      "weight_decay: 0.0001\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "Correlation: -0.3136\n",
      "|Correlation|: 0.3136\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a global seed\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "class MLPEnsemble(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(MLPEnsemble, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, patience, lr, weight_decay, seed):\n",
    "    set_seed(seed)  # Set seed for this training run\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).squeeze()\n",
    "    \n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "    predictions_np = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    \n",
    "    mse = mean_squared_error(y_np, predictions_np)\n",
    "    correlation, _ = pearsonr(y_np, predictions_np)\n",
    "    return mse, correlation\n",
    "\n",
    "def cross_validate_mlp(X, y, n_splits=5, **params):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    mse_scores = []\n",
    "    corr_scores = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    model_params = {\n",
    "        'input_size': X.shape[1],\n",
    "        'hidden_sizes': params['hidden_sizes'],\n",
    "        'output_size': params['output_size'],\n",
    "        'dropout_rate': params['dropout_rate']\n",
    "    }\n",
    "    train_params = {\n",
    "        'epochs': params['epochs'],\n",
    "        'patience': params['patience'],\n",
    "        'lr': params['lr'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'seed': GLOBAL_SEED\n",
    "    }\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        \n",
    "        model = MLPEnsemble(**model_params)\n",
    "        model = train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, **train_params)\n",
    "        \n",
    "        mse, corr = evaluate_model(model, X_val_tensor, y_val_tensor)\n",
    "        mse_scores.append(mse)\n",
    "        corr_scores.append(corr)\n",
    "        \n",
    "        #print(f\"Fold {fold+1} - MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    \n",
    "    return np.mean(mse_scores), np.mean(corr_scores)\n",
    "\n",
    "# Load and preprocess data\n",
    "densenet_df = pd.read_csv('densenet_entorhinal_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "pointnet_df = pd.read_csv('pointnet_entorhinal_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "random_forest_df = pd.read_csv('random_forest_pyradiomics_entorhinal_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "X = np.hstack((\n",
    "    densenet_df.iloc[:, 2:17].values,\n",
    "    pointnet_df.iloc[:, 2:17].values,\n",
    "    random_forest_df.iloc[:, 2:17].values\n",
    "))\n",
    "y = densenet_df['Actual'].values\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32]],\n",
    "    'dropout_rate': [0, 0.3, 0.5],\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "    'epochs': [1000],\n",
    "    'patience': [50],\n",
    "    'output_size': [1]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "for params in product(*param_grid.values()):\n",
    "    config = dict(zip(param_grid.keys(), params))\n",
    "    mse, corr = cross_validate_mlp(X, y, **config)\n",
    "    results.append((config, mse, corr))\n",
    "    #print(f\"Config: {config}\")\n",
    "    #print(f\"MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "# Sort results by MSE (ascending) and absolute correlation (descending)\n",
    "results_by_mse = sorted(results, key=lambda x: x[1])\n",
    "results_by_corr = sorted(results, key=lambda x: abs(x[2]), reverse=True)  # Modified to use absolute value\n",
    "\n",
    "def print_top_5(sorted_results, metric_name):\n",
    "    print(f\"\\nTop 5 configurations by {metric_name}:\")\n",
    "    for i, (config, mse, corr) in enumerate(sorted_results[:5], 1):\n",
    "        print(f\"\\n{i}. Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        print(f\"   MSE: {mse:.4f}\")\n",
    "        print(f\"   Correlation: {corr:.4f}\")\n",
    "        if metric_name == \"Absolute Correlation\":  # Show absolute value for clarity\n",
    "            print(f\"   |Correlation|: {abs(corr):.4f}\")\n",
    "\n",
    "# Print top 5 by MSE\n",
    "print_top_5(results_by_mse, \"MSE\")\n",
    "\n",
    "# Print top 5 by Absolute Correlation\n",
    "print_top_5(results_by_corr, \"Absolute Correlation\")\n",
    "\n",
    "# Best configuration by MSE\n",
    "best_config_mse, best_mse, _ = results_by_mse[0]\n",
    "print(\"\\nBest configuration by MSE:\")\n",
    "for key, value in best_config_mse.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"MSE: {best_mse:.4f}\")\n",
    "\n",
    "# Best configuration by Absolute Correlation\n",
    "best_config_corr, _, best_corr = results_by_corr[0]\n",
    "print(\"\\nBest configuration by Absolute Correlation:\")\n",
    "for key, value in best_config_corr.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Correlation: {best_corr:.4f}\")\n",
    "print(f\"|Correlation|: {abs(best_corr):.4f}\")\n",
    "\n",
    "# Diagnostic: Print predictions vs actual for best absolute correlation model\n",
    "input_size = X.shape[1]\n",
    "best_model = MLPEnsemble(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_config_corr['hidden_sizes'],\n",
    "    output_size=best_config_corr['output_size'],\n",
    "    dropout_rate=best_config_corr['dropout_rate']\n",
    ")\n",
    "best_model = train_model(best_model, torch.FloatTensor(X), torch.FloatTensor(y), torch.FloatTensor(X), torch.FloatTensor(y), \n",
    "                         epochs=best_config_corr['epochs'], patience=best_config_corr['patience'], \n",
    "                         lr=best_config_corr['lr'], weight_decay=best_config_corr['weight_decay'], seed=GLOBAL_SEED)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(torch.FloatTensor(X)).squeeze().numpy()\n",
    "    \n",
    "# print(\"\\nSample of predictions vs actual:\")\n",
    "# for i in range(min(20, len(y))):\n",
    "#     print(f\"Actual: {y[i]:.4f}, Predicted: {predictions[i]:.4f}\")\n",
    "\n",
    "# # Correlation of individual input features with target\n",
    "# print(\"\\nCorrelation of individual features with target:\")\n",
    "# for i in range(X.shape[1]):\n",
    "#     corr, _ = pearsonr(X[:, i], y)\n",
    "#     print(f\"Feature {i+1}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c3cbdf1-c0d2-4f0e-9ba7-38defeba8369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 configurations by MSE:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.0980\n",
      "   Correlation: 0.1363\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [128, 64, 32]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1071\n",
      "   Correlation: -0.0241\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1074\n",
      "   Correlation: 0.0023\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [128]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1091\n",
      "   Correlation: 0.1292\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [128, 64]\n",
      "   dropout_rate: 0.3\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1093\n",
      "   Correlation: 0.0380\n",
      "\n",
      "Top 5 configurations by Absolute Correlation:\n",
      "\n",
      "1. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0\n",
      "   lr: 0.001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.2117\n",
      "   Correlation: -0.3338\n",
      "   |Correlation|: 0.3338\n",
      "\n",
      "2. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0\n",
      "   lr: 0.001\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.2051\n",
      "   Correlation: -0.3225\n",
      "   |Correlation|: 0.3225\n",
      "\n",
      "3. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0\n",
      "   lr: 0.0005\n",
      "   weight_decay: 0.0001\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.2463\n",
      "   Correlation: -0.3147\n",
      "   |Correlation|: 0.3147\n",
      "\n",
      "4. Configuration:\n",
      "   hidden_sizes: [64, 32]\n",
      "   dropout_rate: 0\n",
      "   lr: 0.0005\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.2458\n",
      "   Correlation: -0.3127\n",
      "   |Correlation|: 0.3127\n",
      "\n",
      "5. Configuration:\n",
      "   hidden_sizes: [256]\n",
      "   dropout_rate: 0.5\n",
      "   lr: 0.0001\n",
      "   weight_decay: 1e-05\n",
      "   epochs: 1000\n",
      "   patience: 50\n",
      "   output_size: 1\n",
      "   MSE: 0.1872\n",
      "   Correlation: -0.2884\n",
      "   |Correlation|: 0.2884\n",
      "\n",
      "Best configuration by MSE:\n",
      "hidden_sizes: [128, 64, 32]\n",
      "dropout_rate: 0.5\n",
      "lr: 0.001\n",
      "weight_decay: 0.0001\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "MSE: 0.0980\n",
      "\n",
      "Best configuration by Absolute Correlation:\n",
      "hidden_sizes: [64, 32]\n",
      "dropout_rate: 0\n",
      "lr: 0.001\n",
      "weight_decay: 1e-05\n",
      "epochs: 1000\n",
      "patience: 50\n",
      "output_size: 1\n",
      "Correlation: -0.3338\n",
      "|Correlation|: 0.3338\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a global seed\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "class MLPEnsemble(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(MLPEnsemble, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, patience, lr, weight_decay, seed):\n",
    "    set_seed(seed)  # Set seed for this training run\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).squeeze()\n",
    "    \n",
    "    y_np = y.numpy() if isinstance(y, torch.Tensor) else y\n",
    "    predictions_np = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    \n",
    "    mse = mean_squared_error(y_np, predictions_np)\n",
    "    correlation, _ = pearsonr(y_np, predictions_np)\n",
    "    return mse, correlation\n",
    "\n",
    "def cross_validate_mlp(X, y, n_splits=5, **params):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=GLOBAL_SEED)\n",
    "    mse_scores = []\n",
    "    corr_scores = []\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    model_params = {\n",
    "        'input_size': X.shape[1],\n",
    "        'hidden_sizes': params['hidden_sizes'],\n",
    "        'output_size': params['output_size'],\n",
    "        'dropout_rate': params['dropout_rate']\n",
    "    }\n",
    "    train_params = {\n",
    "        'epochs': params['epochs'],\n",
    "        'patience': params['patience'],\n",
    "        'lr': params['lr'],\n",
    "        'weight_decay': params['weight_decay'],\n",
    "        'seed': GLOBAL_SEED\n",
    "    }\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled)):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.FloatTensor(y_val)\n",
    "        \n",
    "        model = MLPEnsemble(**model_params)\n",
    "        model = train_model(model, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, **train_params)\n",
    "        \n",
    "        mse, corr = evaluate_model(model, X_val_tensor, y_val_tensor)\n",
    "        mse_scores.append(mse)\n",
    "        corr_scores.append(corr)\n",
    "        \n",
    "        #print(f\"Fold {fold+1} - MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    \n",
    "    return np.mean(mse_scores), np.mean(corr_scores)\n",
    "\n",
    "# Load and preprocess data\n",
    "densenet_df = pd.read_csv('densenet_isthmuscingulate_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "pointnet_df = pd.read_csv('pointnet_isthmuscingulate_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "random_forest_df = pd.read_csv('random_forest_pyradiomics_isthmuscingulate_predictions_with_average_and_id.csv').sort_values('ID')\n",
    "\n",
    "X = np.hstack((\n",
    "    densenet_df.iloc[:, 2:17].values,\n",
    "    pointnet_df.iloc[:, 2:17].values,\n",
    "    random_forest_df.iloc[:, 2:17].values\n",
    "))\n",
    "y = densenet_df['Actual'].values\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[64], [128], [256], [64, 32], [128, 64], [256, 128], [128, 64, 32]],\n",
    "    'dropout_rate': [0, 0.3, 0.5],\n",
    "    'lr': [0.001, 0.0005, 0.0001],\n",
    "    'weight_decay': [1e-4, 1e-5],\n",
    "    'epochs': [1000],\n",
    "    'patience': [50],\n",
    "    'output_size': [1]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "results = []\n",
    "for params in product(*param_grid.values()):\n",
    "    config = dict(zip(param_grid.keys(), params))\n",
    "    mse, corr = cross_validate_mlp(X, y, **config)\n",
    "    results.append((config, mse, corr))\n",
    "    #print(f\"Config: {config}\")\n",
    "    #print(f\"MSE: {mse:.4f}, Correlation: {corr:.4f}\")\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "# Sort results by MSE (ascending) and absolute correlation (descending)\n",
    "results_by_mse = sorted(results, key=lambda x: x[1])\n",
    "results_by_corr = sorted(results, key=lambda x: abs(x[2]), reverse=True)  # Modified to use absolute value\n",
    "\n",
    "def print_top_5(sorted_results, metric_name):\n",
    "    print(f\"\\nTop 5 configurations by {metric_name}:\")\n",
    "    for i, (config, mse, corr) in enumerate(sorted_results[:5], 1):\n",
    "        print(f\"\\n{i}. Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        print(f\"   MSE: {mse:.4f}\")\n",
    "        print(f\"   Correlation: {corr:.4f}\")\n",
    "        if metric_name == \"Absolute Correlation\":  # Show absolute value for clarity\n",
    "            print(f\"   |Correlation|: {abs(corr):.4f}\")\n",
    "\n",
    "# Print top 5 by MSE\n",
    "print_top_5(results_by_mse, \"MSE\")\n",
    "\n",
    "# Print top 5 by Absolute Correlation\n",
    "print_top_5(results_by_corr, \"Absolute Correlation\")\n",
    "\n",
    "# Best configuration by MSE\n",
    "best_config_mse, best_mse, _ = results_by_mse[0]\n",
    "print(\"\\nBest configuration by MSE:\")\n",
    "for key, value in best_config_mse.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"MSE: {best_mse:.4f}\")\n",
    "\n",
    "# Best configuration by Absolute Correlation\n",
    "best_config_corr, _, best_corr = results_by_corr[0]\n",
    "print(\"\\nBest configuration by Absolute Correlation:\")\n",
    "for key, value in best_config_corr.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Correlation: {best_corr:.4f}\")\n",
    "print(f\"|Correlation|: {abs(best_corr):.4f}\")\n",
    "\n",
    "# Diagnostic: Print predictions vs actual for best absolute correlation model\n",
    "input_size = X.shape[1]\n",
    "best_model = MLPEnsemble(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_config_corr['hidden_sizes'],\n",
    "    output_size=best_config_corr['output_size'],\n",
    "    dropout_rate=best_config_corr['dropout_rate']\n",
    ")\n",
    "best_model = train_model(best_model, torch.FloatTensor(X), torch.FloatTensor(y), torch.FloatTensor(X), torch.FloatTensor(y), \n",
    "                         epochs=best_config_corr['epochs'], patience=best_config_corr['patience'], \n",
    "                         lr=best_config_corr['lr'], weight_decay=best_config_corr['weight_decay'], seed=GLOBAL_SEED)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(torch.FloatTensor(X)).squeeze().numpy()\n",
    "    \n",
    "# print(\"\\nSample of predictions vs actual:\")\n",
    "# for i in range(min(20, len(y))):\n",
    "#     print(f\"Actual: {y[i]:.4f}, Predicted: {predictions[i]:.4f}\")\n",
    "\n",
    "# # Correlation of individual input features with target\n",
    "# print(\"\\nCorrelation of individual features with target:\")\n",
    "# for i in range(X.shape[1]):\n",
    "#     corr, _ = pearsonr(X[:, i], y)\n",
    "#     print(f\"Feature {i+1}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b1637f-80dd-4b90-a564-3e354c0e85b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
